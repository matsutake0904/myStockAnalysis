{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryo/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/home/ryo/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/home/ryo/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ubiquant_my_library import my_library\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from lightgbm import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path1 = '/home/ryo/Work/Python_Work/kaggle/ubiquent_martket/train_low_memory.parquet'\n",
    "df1 = pd.read_parquet(out_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features0 = [f'f_{i}' for i in range(300)]\n",
    "target = 'target'\n",
    "df_features0 = df1[features0]\n",
    "X_train, X, Y_train, Y = train_test_split(df_features0, df1[target], train_size=0.6, shuffle=False)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X, Y, train_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryo/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.813602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 1884846, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.027871\n",
      "[1]\tvalid_0's l2: 0.873705\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's l2: 0.873521\n",
      "[3]\tvalid_0's l2: 0.873271\n",
      "[4]\tvalid_0's l2: 0.873052\n",
      "[5]\tvalid_0's l2: 0.872854\n",
      "[6]\tvalid_0's l2: 0.872674\n",
      "[7]\tvalid_0's l2: 0.872502\n",
      "[8]\tvalid_0's l2: 0.872357\n",
      "[9]\tvalid_0's l2: 0.872156\n",
      "[10]\tvalid_0's l2: 0.872009\n",
      "[11]\tvalid_0's l2: 0.871858\n",
      "[12]\tvalid_0's l2: 0.87168\n",
      "[13]\tvalid_0's l2: 0.871537\n",
      "[14]\tvalid_0's l2: 0.871376\n",
      "[15]\tvalid_0's l2: 0.871217\n",
      "[16]\tvalid_0's l2: 0.871088\n",
      "[17]\tvalid_0's l2: 0.870898\n",
      "[18]\tvalid_0's l2: 0.870788\n",
      "[19]\tvalid_0's l2: 0.870646\n",
      "[20]\tvalid_0's l2: 0.870517\n",
      "[21]\tvalid_0's l2: 0.870368\n",
      "[22]\tvalid_0's l2: 0.870239\n",
      "[23]\tvalid_0's l2: 0.87012\n",
      "[24]\tvalid_0's l2: 0.870002\n",
      "[25]\tvalid_0's l2: 0.869896\n",
      "[26]\tvalid_0's l2: 0.869801\n",
      "[27]\tvalid_0's l2: 0.869696\n",
      "[28]\tvalid_0's l2: 0.869597\n",
      "[29]\tvalid_0's l2: 0.869478\n",
      "[30]\tvalid_0's l2: 0.869353\n",
      "[31]\tvalid_0's l2: 0.869262\n",
      "[32]\tvalid_0's l2: 0.869163\n",
      "[33]\tvalid_0's l2: 0.869046\n",
      "[34]\tvalid_0's l2: 0.868932\n",
      "[35]\tvalid_0's l2: 0.868821\n",
      "[36]\tvalid_0's l2: 0.868721\n",
      "[37]\tvalid_0's l2: 0.868615\n",
      "[38]\tvalid_0's l2: 0.868551\n",
      "[39]\tvalid_0's l2: 0.868485\n",
      "[40]\tvalid_0's l2: 0.868383\n",
      "[41]\tvalid_0's l2: 0.868296\n",
      "[42]\tvalid_0's l2: 0.868231\n",
      "[43]\tvalid_0's l2: 0.868163\n",
      "[44]\tvalid_0's l2: 0.868095\n",
      "[45]\tvalid_0's l2: 0.868018\n",
      "[46]\tvalid_0's l2: 0.86794\n",
      "[47]\tvalid_0's l2: 0.867871\n",
      "[48]\tvalid_0's l2: 0.867809\n",
      "[49]\tvalid_0's l2: 0.867761\n",
      "[50]\tvalid_0's l2: 0.867715\n",
      "[51]\tvalid_0's l2: 0.867663\n",
      "[52]\tvalid_0's l2: 0.867583\n",
      "[53]\tvalid_0's l2: 0.867534\n",
      "[54]\tvalid_0's l2: 0.867475\n",
      "[55]\tvalid_0's l2: 0.867419\n",
      "[56]\tvalid_0's l2: 0.867365\n",
      "[57]\tvalid_0's l2: 0.867305\n",
      "[58]\tvalid_0's l2: 0.867227\n",
      "[59]\tvalid_0's l2: 0.867159\n",
      "[60]\tvalid_0's l2: 0.867127\n",
      "[61]\tvalid_0's l2: 0.867083\n",
      "[62]\tvalid_0's l2: 0.867019\n",
      "[63]\tvalid_0's l2: 0.866942\n",
      "[64]\tvalid_0's l2: 0.866895\n",
      "[65]\tvalid_0's l2: 0.866823\n",
      "[66]\tvalid_0's l2: 0.866794\n",
      "[67]\tvalid_0's l2: 0.866745\n",
      "[68]\tvalid_0's l2: 0.866703\n",
      "[69]\tvalid_0's l2: 0.866653\n",
      "[70]\tvalid_0's l2: 0.866617\n",
      "[71]\tvalid_0's l2: 0.866565\n",
      "[72]\tvalid_0's l2: 0.866518\n",
      "[73]\tvalid_0's l2: 0.866462\n",
      "[74]\tvalid_0's l2: 0.866412\n",
      "[75]\tvalid_0's l2: 0.866371\n",
      "[76]\tvalid_0's l2: 0.866325\n",
      "[77]\tvalid_0's l2: 0.866272\n",
      "[78]\tvalid_0's l2: 0.866239\n",
      "[79]\tvalid_0's l2: 0.866199\n",
      "[80]\tvalid_0's l2: 0.866172\n",
      "[81]\tvalid_0's l2: 0.866141\n",
      "[82]\tvalid_0's l2: 0.866118\n",
      "[83]\tvalid_0's l2: 0.866096\n",
      "[84]\tvalid_0's l2: 0.866058\n",
      "[85]\tvalid_0's l2: 0.866039\n",
      "[86]\tvalid_0's l2: 0.866005\n",
      "[87]\tvalid_0's l2: 0.865967\n",
      "[88]\tvalid_0's l2: 0.865926\n",
      "[89]\tvalid_0's l2: 0.865886\n",
      "[90]\tvalid_0's l2: 0.86584\n",
      "[91]\tvalid_0's l2: 0.865824\n",
      "[92]\tvalid_0's l2: 0.865798\n",
      "[93]\tvalid_0's l2: 0.86576\n",
      "[94]\tvalid_0's l2: 0.865716\n",
      "[95]\tvalid_0's l2: 0.865692\n",
      "[96]\tvalid_0's l2: 0.865647\n",
      "[97]\tvalid_0's l2: 0.86561\n",
      "[98]\tvalid_0's l2: 0.865571\n",
      "[99]\tvalid_0's l2: 0.865542\n",
      "[100]\tvalid_0's l2: 0.865519\n",
      "[101]\tvalid_0's l2: 0.865502\n",
      "[102]\tvalid_0's l2: 0.865491\n",
      "[103]\tvalid_0's l2: 0.865448\n",
      "[104]\tvalid_0's l2: 0.865414\n",
      "[105]\tvalid_0's l2: 0.8654\n",
      "[106]\tvalid_0's l2: 0.865373\n",
      "[107]\tvalid_0's l2: 0.865335\n",
      "[108]\tvalid_0's l2: 0.8653\n",
      "[109]\tvalid_0's l2: 0.865273\n",
      "[110]\tvalid_0's l2: 0.865234\n",
      "[111]\tvalid_0's l2: 0.865189\n",
      "[112]\tvalid_0's l2: 0.865176\n",
      "[113]\tvalid_0's l2: 0.865155\n",
      "[114]\tvalid_0's l2: 0.865127\n",
      "[115]\tvalid_0's l2: 0.865109\n",
      "[116]\tvalid_0's l2: 0.865078\n",
      "[117]\tvalid_0's l2: 0.865041\n",
      "[118]\tvalid_0's l2: 0.865019\n",
      "[119]\tvalid_0's l2: 0.86499\n",
      "[120]\tvalid_0's l2: 0.864975\n",
      "[121]\tvalid_0's l2: 0.864947\n",
      "[122]\tvalid_0's l2: 0.86492\n",
      "[123]\tvalid_0's l2: 0.86491\n",
      "[124]\tvalid_0's l2: 0.864887\n",
      "[125]\tvalid_0's l2: 0.86488\n",
      "[126]\tvalid_0's l2: 0.864856\n",
      "[127]\tvalid_0's l2: 0.864823\n",
      "[128]\tvalid_0's l2: 0.864791\n",
      "[129]\tvalid_0's l2: 0.864763\n",
      "[130]\tvalid_0's l2: 0.864761\n",
      "[131]\tvalid_0's l2: 0.864721\n",
      "[132]\tvalid_0's l2: 0.864665\n",
      "[133]\tvalid_0's l2: 0.864656\n",
      "[134]\tvalid_0's l2: 0.864631\n",
      "[135]\tvalid_0's l2: 0.864615\n",
      "[136]\tvalid_0's l2: 0.864589\n",
      "[137]\tvalid_0's l2: 0.864557\n",
      "[138]\tvalid_0's l2: 0.864518\n",
      "[139]\tvalid_0's l2: 0.864484\n",
      "[140]\tvalid_0's l2: 0.864453\n",
      "[141]\tvalid_0's l2: 0.864436\n",
      "[142]\tvalid_0's l2: 0.864413\n",
      "[143]\tvalid_0's l2: 0.864391\n",
      "[144]\tvalid_0's l2: 0.864372\n",
      "[145]\tvalid_0's l2: 0.864351\n",
      "[146]\tvalid_0's l2: 0.864319\n",
      "[147]\tvalid_0's l2: 0.864305\n",
      "[148]\tvalid_0's l2: 0.864286\n",
      "[149]\tvalid_0's l2: 0.864269\n",
      "[150]\tvalid_0's l2: 0.864246\n",
      "[151]\tvalid_0's l2: 0.864233\n",
      "[152]\tvalid_0's l2: 0.864224\n",
      "[153]\tvalid_0's l2: 0.864192\n",
      "[154]\tvalid_0's l2: 0.864168\n",
      "[155]\tvalid_0's l2: 0.864144\n",
      "[156]\tvalid_0's l2: 0.864129\n",
      "[157]\tvalid_0's l2: 0.864097\n",
      "[158]\tvalid_0's l2: 0.864079\n",
      "[159]\tvalid_0's l2: 0.864085\n",
      "[160]\tvalid_0's l2: 0.86405\n",
      "[161]\tvalid_0's l2: 0.864036\n",
      "[162]\tvalid_0's l2: 0.864009\n",
      "[163]\tvalid_0's l2: 0.863982\n",
      "[164]\tvalid_0's l2: 0.863961\n",
      "[165]\tvalid_0's l2: 0.863943\n",
      "[166]\tvalid_0's l2: 0.863923\n",
      "[167]\tvalid_0's l2: 0.863895\n",
      "[168]\tvalid_0's l2: 0.863869\n",
      "[169]\tvalid_0's l2: 0.863854\n",
      "[170]\tvalid_0's l2: 0.863834\n",
      "[171]\tvalid_0's l2: 0.863821\n",
      "[172]\tvalid_0's l2: 0.863808\n",
      "[173]\tvalid_0's l2: 0.863787\n",
      "[174]\tvalid_0's l2: 0.863766\n",
      "[175]\tvalid_0's l2: 0.863753\n",
      "[176]\tvalid_0's l2: 0.863734\n",
      "[177]\tvalid_0's l2: 0.863689\n",
      "[178]\tvalid_0's l2: 0.863672\n",
      "[179]\tvalid_0's l2: 0.863639\n",
      "[180]\tvalid_0's l2: 0.86364\n",
      "[181]\tvalid_0's l2: 0.86363\n",
      "[182]\tvalid_0's l2: 0.863595\n",
      "[183]\tvalid_0's l2: 0.863574\n",
      "[184]\tvalid_0's l2: 0.863548\n",
      "[185]\tvalid_0's l2: 0.863547\n",
      "[186]\tvalid_0's l2: 0.86353\n",
      "[187]\tvalid_0's l2: 0.863504\n",
      "[188]\tvalid_0's l2: 0.863477\n",
      "[189]\tvalid_0's l2: 0.863448\n",
      "[190]\tvalid_0's l2: 0.863419\n",
      "[191]\tvalid_0's l2: 0.863413\n",
      "[192]\tvalid_0's l2: 0.863388\n",
      "[193]\tvalid_0's l2: 0.863374\n",
      "[194]\tvalid_0's l2: 0.863354\n",
      "[195]\tvalid_0's l2: 0.863327\n",
      "[196]\tvalid_0's l2: 0.863318\n",
      "[197]\tvalid_0's l2: 0.863297\n",
      "[198]\tvalid_0's l2: 0.863282\n",
      "[199]\tvalid_0's l2: 0.863272\n",
      "[200]\tvalid_0's l2: 0.863276\n",
      "[201]\tvalid_0's l2: 0.86324\n",
      "[202]\tvalid_0's l2: 0.863231\n",
      "[203]\tvalid_0's l2: 0.863208\n",
      "[204]\tvalid_0's l2: 0.863188\n",
      "[205]\tvalid_0's l2: 0.863182\n",
      "[206]\tvalid_0's l2: 0.863191\n",
      "[207]\tvalid_0's l2: 0.863172\n",
      "[208]\tvalid_0's l2: 0.863154\n",
      "[209]\tvalid_0's l2: 0.863133\n",
      "[210]\tvalid_0's l2: 0.86311\n",
      "[211]\tvalid_0's l2: 0.86308\n",
      "[212]\tvalid_0's l2: 0.863069\n",
      "[213]\tvalid_0's l2: 0.863039\n",
      "[214]\tvalid_0's l2: 0.863026\n",
      "[215]\tvalid_0's l2: 0.863015\n",
      "[216]\tvalid_0's l2: 0.86301\n",
      "[217]\tvalid_0's l2: 0.863006\n",
      "[218]\tvalid_0's l2: 0.862998\n",
      "[219]\tvalid_0's l2: 0.862992\n",
      "[220]\tvalid_0's l2: 0.862989\n",
      "[221]\tvalid_0's l2: 0.862984\n",
      "[222]\tvalid_0's l2: 0.86297\n",
      "[223]\tvalid_0's l2: 0.862952\n",
      "[224]\tvalid_0's l2: 0.862926\n",
      "[225]\tvalid_0's l2: 0.862923\n",
      "[226]\tvalid_0's l2: 0.862913\n",
      "[227]\tvalid_0's l2: 0.862913\n",
      "[228]\tvalid_0's l2: 0.862888\n",
      "[229]\tvalid_0's l2: 0.862868\n",
      "[230]\tvalid_0's l2: 0.862864\n",
      "[231]\tvalid_0's l2: 0.862852\n",
      "[232]\tvalid_0's l2: 0.862844\n",
      "[233]\tvalid_0's l2: 0.862821\n",
      "[234]\tvalid_0's l2: 0.862803\n",
      "[235]\tvalid_0's l2: 0.8628\n",
      "[236]\tvalid_0's l2: 0.862796\n",
      "[237]\tvalid_0's l2: 0.862783\n",
      "[238]\tvalid_0's l2: 0.862762\n",
      "[239]\tvalid_0's l2: 0.862757\n",
      "[240]\tvalid_0's l2: 0.862742\n",
      "[241]\tvalid_0's l2: 0.862744\n",
      "[242]\tvalid_0's l2: 0.862734\n",
      "[243]\tvalid_0's l2: 0.862734\n",
      "[244]\tvalid_0's l2: 0.862712\n",
      "[245]\tvalid_0's l2: 0.862697\n",
      "[246]\tvalid_0's l2: 0.862686\n",
      "[247]\tvalid_0's l2: 0.862686\n",
      "[248]\tvalid_0's l2: 0.86266\n",
      "[249]\tvalid_0's l2: 0.862641\n",
      "[250]\tvalid_0's l2: 0.862626\n",
      "[251]\tvalid_0's l2: 0.862606\n",
      "[252]\tvalid_0's l2: 0.862599\n",
      "[253]\tvalid_0's l2: 0.86259\n",
      "[254]\tvalid_0's l2: 0.862578\n",
      "[255]\tvalid_0's l2: 0.862569\n",
      "[256]\tvalid_0's l2: 0.862556\n",
      "[257]\tvalid_0's l2: 0.862549\n",
      "[258]\tvalid_0's l2: 0.862557\n",
      "[259]\tvalid_0's l2: 0.862543\n",
      "[260]\tvalid_0's l2: 0.862529\n",
      "[261]\tvalid_0's l2: 0.862498\n",
      "[262]\tvalid_0's l2: 0.862492\n",
      "[263]\tvalid_0's l2: 0.86249\n",
      "[264]\tvalid_0's l2: 0.862463\n",
      "[265]\tvalid_0's l2: 0.862444\n",
      "[266]\tvalid_0's l2: 0.862428\n",
      "[267]\tvalid_0's l2: 0.86241\n",
      "[268]\tvalid_0's l2: 0.862417\n",
      "[269]\tvalid_0's l2: 0.862406\n",
      "[270]\tvalid_0's l2: 0.862407\n",
      "[271]\tvalid_0's l2: 0.862394\n",
      "[272]\tvalid_0's l2: 0.862366\n",
      "[273]\tvalid_0's l2: 0.862357\n",
      "[274]\tvalid_0's l2: 0.862342\n",
      "[275]\tvalid_0's l2: 0.862315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276]\tvalid_0's l2: 0.862304\n",
      "[277]\tvalid_0's l2: 0.862309\n",
      "[278]\tvalid_0's l2: 0.862285\n",
      "[279]\tvalid_0's l2: 0.862265\n",
      "[280]\tvalid_0's l2: 0.862238\n",
      "[281]\tvalid_0's l2: 0.862239\n",
      "[282]\tvalid_0's l2: 0.862245\n",
      "[283]\tvalid_0's l2: 0.862231\n",
      "[284]\tvalid_0's l2: 0.862228\n",
      "[285]\tvalid_0's l2: 0.862227\n",
      "[286]\tvalid_0's l2: 0.862219\n",
      "[287]\tvalid_0's l2: 0.86218\n",
      "[288]\tvalid_0's l2: 0.86216\n",
      "[289]\tvalid_0's l2: 0.862143\n",
      "[290]\tvalid_0's l2: 0.862134\n",
      "[291]\tvalid_0's l2: 0.86214\n",
      "[292]\tvalid_0's l2: 0.862136\n",
      "[293]\tvalid_0's l2: 0.862135\n",
      "[294]\tvalid_0's l2: 0.862113\n",
      "[295]\tvalid_0's l2: 0.86211\n",
      "[296]\tvalid_0's l2: 0.862092\n",
      "[297]\tvalid_0's l2: 0.862075\n",
      "[298]\tvalid_0's l2: 0.862058\n",
      "[299]\tvalid_0's l2: 0.862049\n",
      "[300]\tvalid_0's l2: 0.862051\n",
      "[301]\tvalid_0's l2: 0.862029\n",
      "[302]\tvalid_0's l2: 0.862017\n",
      "[303]\tvalid_0's l2: 0.862008\n",
      "[304]\tvalid_0's l2: 0.861998\n",
      "[305]\tvalid_0's l2: 0.861979\n",
      "[306]\tvalid_0's l2: 0.861967\n",
      "[307]\tvalid_0's l2: 0.861944\n",
      "[308]\tvalid_0's l2: 0.861949\n",
      "[309]\tvalid_0's l2: 0.861932\n",
      "[310]\tvalid_0's l2: 0.861925\n",
      "[311]\tvalid_0's l2: 0.861917\n",
      "[312]\tvalid_0's l2: 0.861915\n",
      "[313]\tvalid_0's l2: 0.861905\n",
      "[314]\tvalid_0's l2: 0.861894\n",
      "[315]\tvalid_0's l2: 0.86188\n",
      "[316]\tvalid_0's l2: 0.861864\n",
      "[317]\tvalid_0's l2: 0.861843\n",
      "[318]\tvalid_0's l2: 0.861821\n",
      "[319]\tvalid_0's l2: 0.861819\n",
      "[320]\tvalid_0's l2: 0.861808\n",
      "[321]\tvalid_0's l2: 0.861794\n",
      "[322]\tvalid_0's l2: 0.861772\n",
      "[323]\tvalid_0's l2: 0.861779\n",
      "[324]\tvalid_0's l2: 0.86176\n",
      "[325]\tvalid_0's l2: 0.861749\n",
      "[326]\tvalid_0's l2: 0.861732\n",
      "[327]\tvalid_0's l2: 0.861727\n",
      "[328]\tvalid_0's l2: 0.861719\n",
      "[329]\tvalid_0's l2: 0.861704\n",
      "[330]\tvalid_0's l2: 0.861689\n",
      "[331]\tvalid_0's l2: 0.861674\n",
      "[332]\tvalid_0's l2: 0.861671\n",
      "[333]\tvalid_0's l2: 0.861668\n",
      "[334]\tvalid_0's l2: 0.861653\n",
      "[335]\tvalid_0's l2: 0.861638\n",
      "[336]\tvalid_0's l2: 0.861616\n",
      "[337]\tvalid_0's l2: 0.861603\n",
      "[338]\tvalid_0's l2: 0.861614\n",
      "[339]\tvalid_0's l2: 0.861606\n",
      "[340]\tvalid_0's l2: 0.8616\n",
      "[341]\tvalid_0's l2: 0.861599\n",
      "[342]\tvalid_0's l2: 0.861584\n",
      "[343]\tvalid_0's l2: 0.861582\n",
      "[344]\tvalid_0's l2: 0.861589\n",
      "[345]\tvalid_0's l2: 0.861577\n",
      "[346]\tvalid_0's l2: 0.861568\n",
      "[347]\tvalid_0's l2: 0.861556\n",
      "[348]\tvalid_0's l2: 0.861552\n",
      "[349]\tvalid_0's l2: 0.86154\n",
      "[350]\tvalid_0's l2: 0.861531\n",
      "[351]\tvalid_0's l2: 0.861522\n",
      "[352]\tvalid_0's l2: 0.861525\n",
      "[353]\tvalid_0's l2: 0.861523\n",
      "[354]\tvalid_0's l2: 0.86152\n",
      "[355]\tvalid_0's l2: 0.861513\n",
      "[356]\tvalid_0's l2: 0.861499\n",
      "[357]\tvalid_0's l2: 0.861495\n",
      "[358]\tvalid_0's l2: 0.861479\n",
      "[359]\tvalid_0's l2: 0.861492\n",
      "[360]\tvalid_0's l2: 0.861503\n",
      "[361]\tvalid_0's l2: 0.861506\n",
      "[362]\tvalid_0's l2: 0.861501\n",
      "[363]\tvalid_0's l2: 0.861508\n",
      "[364]\tvalid_0's l2: 0.86151\n",
      "[365]\tvalid_0's l2: 0.86151\n",
      "[366]\tvalid_0's l2: 0.861523\n",
      "[367]\tvalid_0's l2: 0.861509\n",
      "[368]\tvalid_0's l2: 0.861499\n",
      "Early stopping, best iteration is:\n",
      "[358]\tvalid_0's l2: 0.861479\n",
      "Validation Pearsonr score : 0.1177\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n",
    "params = {'seed': 1,\n",
    "           'objective': \"regression\",\n",
    "           'learning_rate': 0.02,\n",
    "           'bagging_fraction': 0.2,\n",
    "           'bagging_freq': 1,\n",
    "           'feature_fraction': 0.3,\n",
    "           'max_depth': 10,\n",
    "           'min_child_samples': 50,\n",
    "           'num_leaves': 64}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=19450815,\n",
    "                valid_sets=lgb_eval,\n",
    "                #verbose_eval=False,\n",
    "                early_stopping_rounds=10,\n",
    "                )\n",
    "\n",
    "Y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "score_tuple = pearsonr(Y_test, Y_pred)\n",
    "score = score_tuple[0]\n",
    "print(f\"Validation Pearsonr score : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path2 = '/home/ryo/Work/Python_Work/kaggle/ubiquent_martket/train_low_memory_diffAve.parquet'\n",
    "df3 = pd.read_parquet(out_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____add_diff_average_columns_____\n"
     ]
    }
   ],
   "source": [
    "print(\"____add_diff_average_columns_____\")\n",
    "features = [f'f_{i}' for i in range(300)]\n",
    "features.append('diff_average')\n",
    "target = 'target'\n",
    "df_features = df3[features]\n",
    "X_train, X, Y_train, Y = train_test_split(df_features, df3[target], train_size=0.6, shuffle=False)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X, Y, train_size=0.5, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryo/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.957655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76755\n",
      "[LightGBM] [Info] Number of data points in the train set: 1884846, number of used features: 301\n",
      "[LightGBM] [Info] Start training from score -0.027871\n",
      "[1]\tvalid_0's l2: 0.873711\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's l2: 0.873495\n",
      "[3]\tvalid_0's l2: 0.87326\n",
      "[4]\tvalid_0's l2: 0.873086\n",
      "[5]\tvalid_0's l2: 0.872833\n",
      "[6]\tvalid_0's l2: 0.872665\n",
      "[7]\tvalid_0's l2: 0.872488\n",
      "[8]\tvalid_0's l2: 0.872326\n",
      "[9]\tvalid_0's l2: 0.872171\n",
      "[10]\tvalid_0's l2: 0.872016\n",
      "[11]\tvalid_0's l2: 0.87185\n",
      "[12]\tvalid_0's l2: 0.871667\n",
      "[13]\tvalid_0's l2: 0.871539\n",
      "[14]\tvalid_0's l2: 0.871381\n",
      "[15]\tvalid_0's l2: 0.871235\n",
      "[16]\tvalid_0's l2: 0.87109\n",
      "[17]\tvalid_0's l2: 0.870938\n",
      "[18]\tvalid_0's l2: 0.870793\n",
      "[19]\tvalid_0's l2: 0.870619\n",
      "[20]\tvalid_0's l2: 0.870458\n",
      "[21]\tvalid_0's l2: 0.870311\n",
      "[22]\tvalid_0's l2: 0.870234\n",
      "[23]\tvalid_0's l2: 0.870101\n",
      "[24]\tvalid_0's l2: 0.869984\n",
      "[25]\tvalid_0's l2: 0.869885\n",
      "[26]\tvalid_0's l2: 0.869782\n",
      "[27]\tvalid_0's l2: 0.869681\n",
      "[28]\tvalid_0's l2: 0.869575\n",
      "[29]\tvalid_0's l2: 0.869497\n",
      "[30]\tvalid_0's l2: 0.869386\n",
      "[31]\tvalid_0's l2: 0.869277\n",
      "[32]\tvalid_0's l2: 0.869156\n",
      "[33]\tvalid_0's l2: 0.869038\n",
      "[34]\tvalid_0's l2: 0.86894\n",
      "[35]\tvalid_0's l2: 0.86887\n",
      "[36]\tvalid_0's l2: 0.868745\n",
      "[37]\tvalid_0's l2: 0.86867\n",
      "[38]\tvalid_0's l2: 0.868576\n",
      "[39]\tvalid_0's l2: 0.868469\n",
      "[40]\tvalid_0's l2: 0.868374\n",
      "[41]\tvalid_0's l2: 0.868306\n",
      "[42]\tvalid_0's l2: 0.868213\n",
      "[43]\tvalid_0's l2: 0.868138\n",
      "[44]\tvalid_0's l2: 0.868062\n",
      "[45]\tvalid_0's l2: 0.867987\n",
      "[46]\tvalid_0's l2: 0.867901\n",
      "[47]\tvalid_0's l2: 0.867856\n",
      "[48]\tvalid_0's l2: 0.867818\n",
      "[49]\tvalid_0's l2: 0.867728\n",
      "[50]\tvalid_0's l2: 0.867653\n",
      "[51]\tvalid_0's l2: 0.867582\n",
      "[52]\tvalid_0's l2: 0.86753\n",
      "[53]\tvalid_0's l2: 0.867485\n",
      "[54]\tvalid_0's l2: 0.867439\n",
      "[55]\tvalid_0's l2: 0.867386\n",
      "[56]\tvalid_0's l2: 0.867324\n",
      "[57]\tvalid_0's l2: 0.867281\n",
      "[58]\tvalid_0's l2: 0.867216\n",
      "[59]\tvalid_0's l2: 0.867165\n",
      "[60]\tvalid_0's l2: 0.867116\n",
      "[61]\tvalid_0's l2: 0.867083\n",
      "[62]\tvalid_0's l2: 0.867046\n",
      "[63]\tvalid_0's l2: 0.866988\n",
      "[64]\tvalid_0's l2: 0.866899\n",
      "[65]\tvalid_0's l2: 0.866861\n",
      "[66]\tvalid_0's l2: 0.866824\n",
      "[67]\tvalid_0's l2: 0.866777\n",
      "[68]\tvalid_0's l2: 0.866732\n",
      "[69]\tvalid_0's l2: 0.866674\n",
      "[70]\tvalid_0's l2: 0.866623\n",
      "[71]\tvalid_0's l2: 0.86656\n",
      "[72]\tvalid_0's l2: 0.866535\n",
      "[73]\tvalid_0's l2: 0.866487\n",
      "[74]\tvalid_0's l2: 0.866437\n",
      "[75]\tvalid_0's l2: 0.866395\n",
      "[76]\tvalid_0's l2: 0.86636\n",
      "[77]\tvalid_0's l2: 0.866325\n",
      "[78]\tvalid_0's l2: 0.86627\n",
      "[79]\tvalid_0's l2: 0.866233\n",
      "[80]\tvalid_0's l2: 0.866191\n",
      "[81]\tvalid_0's l2: 0.866161\n",
      "[82]\tvalid_0's l2: 0.866135\n",
      "[83]\tvalid_0's l2: 0.866124\n",
      "[84]\tvalid_0's l2: 0.866066\n",
      "[85]\tvalid_0's l2: 0.866052\n",
      "[86]\tvalid_0's l2: 0.866021\n",
      "[87]\tvalid_0's l2: 0.865985\n",
      "[88]\tvalid_0's l2: 0.865947\n",
      "[89]\tvalid_0's l2: 0.865893\n",
      "[90]\tvalid_0's l2: 0.865868\n",
      "[91]\tvalid_0's l2: 0.865834\n",
      "[92]\tvalid_0's l2: 0.865786\n",
      "[93]\tvalid_0's l2: 0.865748\n",
      "[94]\tvalid_0's l2: 0.865722\n",
      "[95]\tvalid_0's l2: 0.865701\n",
      "[96]\tvalid_0's l2: 0.865676\n",
      "[97]\tvalid_0's l2: 0.865646\n",
      "[98]\tvalid_0's l2: 0.865608\n",
      "[99]\tvalid_0's l2: 0.865595\n",
      "[100]\tvalid_0's l2: 0.865567\n",
      "[101]\tvalid_0's l2: 0.865528\n",
      "[102]\tvalid_0's l2: 0.8655\n",
      "[103]\tvalid_0's l2: 0.865492\n",
      "[104]\tvalid_0's l2: 0.865455\n",
      "[105]\tvalid_0's l2: 0.865427\n",
      "[106]\tvalid_0's l2: 0.865388\n",
      "[107]\tvalid_0's l2: 0.865367\n",
      "[108]\tvalid_0's l2: 0.865331\n",
      "[109]\tvalid_0's l2: 0.86532\n",
      "[110]\tvalid_0's l2: 0.865307\n",
      "[111]\tvalid_0's l2: 0.865299\n",
      "[112]\tvalid_0's l2: 0.865286\n",
      "[113]\tvalid_0's l2: 0.865252\n",
      "[114]\tvalid_0's l2: 0.865211\n",
      "[115]\tvalid_0's l2: 0.865189\n",
      "[116]\tvalid_0's l2: 0.865163\n",
      "[117]\tvalid_0's l2: 0.865147\n",
      "[118]\tvalid_0's l2: 0.865124\n",
      "[119]\tvalid_0's l2: 0.865081\n",
      "[120]\tvalid_0's l2: 0.865011\n",
      "[121]\tvalid_0's l2: 0.864973\n",
      "[122]\tvalid_0's l2: 0.864938\n",
      "[123]\tvalid_0's l2: 0.86493\n",
      "[124]\tvalid_0's l2: 0.864917\n",
      "[125]\tvalid_0's l2: 0.864885\n",
      "[126]\tvalid_0's l2: 0.864846\n",
      "[127]\tvalid_0's l2: 0.864825\n",
      "[128]\tvalid_0's l2: 0.864798\n",
      "[129]\tvalid_0's l2: 0.864804\n",
      "[130]\tvalid_0's l2: 0.864787\n",
      "[131]\tvalid_0's l2: 0.864753\n",
      "[132]\tvalid_0's l2: 0.864726\n",
      "[133]\tvalid_0's l2: 0.864705\n",
      "[134]\tvalid_0's l2: 0.864661\n",
      "[135]\tvalid_0's l2: 0.864637\n",
      "[136]\tvalid_0's l2: 0.864576\n",
      "[137]\tvalid_0's l2: 0.864553\n",
      "[138]\tvalid_0's l2: 0.864531\n",
      "[139]\tvalid_0's l2: 0.864505\n",
      "[140]\tvalid_0's l2: 0.864482\n",
      "[141]\tvalid_0's l2: 0.864471\n",
      "[142]\tvalid_0's l2: 0.864452\n",
      "[143]\tvalid_0's l2: 0.864427\n",
      "[144]\tvalid_0's l2: 0.864388\n",
      "[145]\tvalid_0's l2: 0.864372\n",
      "[146]\tvalid_0's l2: 0.864327\n",
      "[147]\tvalid_0's l2: 0.864306\n",
      "[148]\tvalid_0's l2: 0.864282\n",
      "[149]\tvalid_0's l2: 0.864273\n",
      "[150]\tvalid_0's l2: 0.864249\n",
      "[151]\tvalid_0's l2: 0.864227\n",
      "[152]\tvalid_0's l2: 0.864215\n",
      "[153]\tvalid_0's l2: 0.864203\n",
      "[154]\tvalid_0's l2: 0.864202\n",
      "[155]\tvalid_0's l2: 0.864163\n",
      "[156]\tvalid_0's l2: 0.864146\n",
      "[157]\tvalid_0's l2: 0.864117\n",
      "[158]\tvalid_0's l2: 0.864095\n",
      "[159]\tvalid_0's l2: 0.864077\n",
      "[160]\tvalid_0's l2: 0.864062\n",
      "[161]\tvalid_0's l2: 0.864036\n",
      "[162]\tvalid_0's l2: 0.864029\n",
      "[163]\tvalid_0's l2: 0.864003\n",
      "[164]\tvalid_0's l2: 0.863981\n",
      "[165]\tvalid_0's l2: 0.863972\n",
      "[166]\tvalid_0's l2: 0.863953\n",
      "[167]\tvalid_0's l2: 0.863916\n",
      "[168]\tvalid_0's l2: 0.863878\n",
      "[169]\tvalid_0's l2: 0.863869\n",
      "[170]\tvalid_0's l2: 0.863853\n",
      "[171]\tvalid_0's l2: 0.863851\n",
      "[172]\tvalid_0's l2: 0.863827\n",
      "[173]\tvalid_0's l2: 0.863818\n",
      "[174]\tvalid_0's l2: 0.863803\n",
      "[175]\tvalid_0's l2: 0.86379\n",
      "[176]\tvalid_0's l2: 0.863771\n",
      "[177]\tvalid_0's l2: 0.863746\n",
      "[178]\tvalid_0's l2: 0.863737\n",
      "[179]\tvalid_0's l2: 0.863723\n",
      "[180]\tvalid_0's l2: 0.863706\n",
      "[181]\tvalid_0's l2: 0.863703\n",
      "[182]\tvalid_0's l2: 0.863687\n",
      "[183]\tvalid_0's l2: 0.863668\n",
      "[184]\tvalid_0's l2: 0.863656\n",
      "[185]\tvalid_0's l2: 0.863634\n",
      "[186]\tvalid_0's l2: 0.863622\n",
      "[187]\tvalid_0's l2: 0.863584\n",
      "[188]\tvalid_0's l2: 0.863574\n",
      "[189]\tvalid_0's l2: 0.863559\n",
      "[190]\tvalid_0's l2: 0.863526\n",
      "[191]\tvalid_0's l2: 0.863503\n",
      "[192]\tvalid_0's l2: 0.863474\n",
      "[193]\tvalid_0's l2: 0.863456\n",
      "[194]\tvalid_0's l2: 0.863418\n",
      "[195]\tvalid_0's l2: 0.863392\n",
      "[196]\tvalid_0's l2: 0.863371\n",
      "[197]\tvalid_0's l2: 0.863349\n",
      "[198]\tvalid_0's l2: 0.863322\n",
      "[199]\tvalid_0's l2: 0.863303\n",
      "[200]\tvalid_0's l2: 0.863297\n",
      "[201]\tvalid_0's l2: 0.863289\n",
      "[202]\tvalid_0's l2: 0.863278\n",
      "[203]\tvalid_0's l2: 0.86328\n",
      "[204]\tvalid_0's l2: 0.863267\n",
      "[205]\tvalid_0's l2: 0.863237\n",
      "[206]\tvalid_0's l2: 0.863227\n",
      "[207]\tvalid_0's l2: 0.863205\n",
      "[208]\tvalid_0's l2: 0.863173\n",
      "[209]\tvalid_0's l2: 0.863155\n",
      "[210]\tvalid_0's l2: 0.863134\n",
      "[211]\tvalid_0's l2: 0.863122\n",
      "[212]\tvalid_0's l2: 0.863111\n",
      "[213]\tvalid_0's l2: 0.863099\n",
      "[214]\tvalid_0's l2: 0.863046\n",
      "[215]\tvalid_0's l2: 0.863023\n",
      "[216]\tvalid_0's l2: 0.863012\n",
      "[217]\tvalid_0's l2: 0.862999\n",
      "[218]\tvalid_0's l2: 0.862973\n",
      "[219]\tvalid_0's l2: 0.862963\n",
      "[220]\tvalid_0's l2: 0.862959\n",
      "[221]\tvalid_0's l2: 0.862945\n",
      "[222]\tvalid_0's l2: 0.862922\n",
      "[223]\tvalid_0's l2: 0.862918\n",
      "[224]\tvalid_0's l2: 0.862892\n",
      "[225]\tvalid_0's l2: 0.862883\n",
      "[226]\tvalid_0's l2: 0.862876\n",
      "[227]\tvalid_0's l2: 0.862847\n",
      "[228]\tvalid_0's l2: 0.86281\n",
      "[229]\tvalid_0's l2: 0.862781\n",
      "[230]\tvalid_0's l2: 0.862787\n",
      "[231]\tvalid_0's l2: 0.862783\n",
      "[232]\tvalid_0's l2: 0.862756\n",
      "[233]\tvalid_0's l2: 0.862762\n",
      "[234]\tvalid_0's l2: 0.862758\n",
      "[235]\tvalid_0's l2: 0.862734\n",
      "[236]\tvalid_0's l2: 0.862726\n",
      "[237]\tvalid_0's l2: 0.862714\n",
      "[238]\tvalid_0's l2: 0.862702\n",
      "[239]\tvalid_0's l2: 0.862711\n",
      "[240]\tvalid_0's l2: 0.862693\n",
      "[241]\tvalid_0's l2: 0.862683\n",
      "[242]\tvalid_0's l2: 0.86267\n",
      "[243]\tvalid_0's l2: 0.862667\n",
      "[244]\tvalid_0's l2: 0.862662\n",
      "[245]\tvalid_0's l2: 0.862685\n",
      "[246]\tvalid_0's l2: 0.862658\n",
      "[247]\tvalid_0's l2: 0.862638\n",
      "[248]\tvalid_0's l2: 0.862622\n",
      "[249]\tvalid_0's l2: 0.86261\n",
      "[250]\tvalid_0's l2: 0.862604\n",
      "[251]\tvalid_0's l2: 0.862586\n",
      "[252]\tvalid_0's l2: 0.862559\n",
      "[253]\tvalid_0's l2: 0.862527\n",
      "[254]\tvalid_0's l2: 0.862507\n",
      "[255]\tvalid_0's l2: 0.862488\n",
      "[256]\tvalid_0's l2: 0.862493\n",
      "[257]\tvalid_0's l2: 0.862482\n",
      "[258]\tvalid_0's l2: 0.862472\n",
      "[259]\tvalid_0's l2: 0.862472\n",
      "[260]\tvalid_0's l2: 0.862453\n",
      "[261]\tvalid_0's l2: 0.862458\n",
      "[262]\tvalid_0's l2: 0.862456\n",
      "[263]\tvalid_0's l2: 0.86246\n",
      "[264]\tvalid_0's l2: 0.862446\n",
      "[265]\tvalid_0's l2: 0.862426\n",
      "[266]\tvalid_0's l2: 0.862418\n",
      "[267]\tvalid_0's l2: 0.862403\n",
      "[268]\tvalid_0's l2: 0.862404\n",
      "[269]\tvalid_0's l2: 0.862387\n",
      "[270]\tvalid_0's l2: 0.862377\n",
      "[271]\tvalid_0's l2: 0.86236\n",
      "[272]\tvalid_0's l2: 0.862359\n",
      "[273]\tvalid_0's l2: 0.862355\n",
      "[274]\tvalid_0's l2: 0.862344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275]\tvalid_0's l2: 0.862337\n",
      "[276]\tvalid_0's l2: 0.862334\n",
      "[277]\tvalid_0's l2: 0.862336\n",
      "[278]\tvalid_0's l2: 0.86234\n",
      "[279]\tvalid_0's l2: 0.862335\n",
      "[280]\tvalid_0's l2: 0.862313\n",
      "[281]\tvalid_0's l2: 0.862322\n",
      "[282]\tvalid_0's l2: 0.86231\n",
      "[283]\tvalid_0's l2: 0.86231\n",
      "[284]\tvalid_0's l2: 0.86232\n",
      "[285]\tvalid_0's l2: 0.862317\n",
      "[286]\tvalid_0's l2: 0.862319\n",
      "[287]\tvalid_0's l2: 0.862311\n",
      "[288]\tvalid_0's l2: 0.862299\n",
      "[289]\tvalid_0's l2: 0.862272\n",
      "[290]\tvalid_0's l2: 0.86227\n",
      "[291]\tvalid_0's l2: 0.862262\n",
      "[292]\tvalid_0's l2: 0.862259\n",
      "[293]\tvalid_0's l2: 0.862251\n",
      "[294]\tvalid_0's l2: 0.862215\n",
      "[295]\tvalid_0's l2: 0.862208\n",
      "[296]\tvalid_0's l2: 0.862193\n",
      "[297]\tvalid_0's l2: 0.862181\n",
      "[298]\tvalid_0's l2: 0.862176\n",
      "[299]\tvalid_0's l2: 0.86216\n",
      "[300]\tvalid_0's l2: 0.862139\n",
      "[301]\tvalid_0's l2: 0.862129\n",
      "[302]\tvalid_0's l2: 0.862117\n",
      "[303]\tvalid_0's l2: 0.862121\n",
      "[304]\tvalid_0's l2: 0.8621\n",
      "[305]\tvalid_0's l2: 0.862102\n",
      "[306]\tvalid_0's l2: 0.862088\n",
      "[307]\tvalid_0's l2: 0.86206\n",
      "[308]\tvalid_0's l2: 0.862024\n",
      "[309]\tvalid_0's l2: 0.862016\n",
      "[310]\tvalid_0's l2: 0.862014\n",
      "[311]\tvalid_0's l2: 0.862004\n",
      "[312]\tvalid_0's l2: 0.861974\n",
      "[313]\tvalid_0's l2: 0.861954\n",
      "[314]\tvalid_0's l2: 0.86194\n",
      "[315]\tvalid_0's l2: 0.861925\n",
      "[316]\tvalid_0's l2: 0.861902\n",
      "[317]\tvalid_0's l2: 0.861893\n",
      "[318]\tvalid_0's l2: 0.861882\n",
      "[319]\tvalid_0's l2: 0.86187\n",
      "[320]\tvalid_0's l2: 0.861866\n",
      "[321]\tvalid_0's l2: 0.861843\n",
      "[322]\tvalid_0's l2: 0.861833\n",
      "[323]\tvalid_0's l2: 0.861836\n",
      "[324]\tvalid_0's l2: 0.861815\n",
      "[325]\tvalid_0's l2: 0.861812\n",
      "[326]\tvalid_0's l2: 0.861798\n",
      "[327]\tvalid_0's l2: 0.861805\n",
      "[328]\tvalid_0's l2: 0.861805\n",
      "[329]\tvalid_0's l2: 0.861792\n",
      "[330]\tvalid_0's l2: 0.861783\n",
      "[331]\tvalid_0's l2: 0.861759\n",
      "[332]\tvalid_0's l2: 0.861761\n",
      "[333]\tvalid_0's l2: 0.861755\n",
      "[334]\tvalid_0's l2: 0.861748\n",
      "[335]\tvalid_0's l2: 0.861733\n",
      "[336]\tvalid_0's l2: 0.861733\n",
      "[337]\tvalid_0's l2: 0.861729\n",
      "[338]\tvalid_0's l2: 0.86172\n",
      "[339]\tvalid_0's l2: 0.861702\n",
      "[340]\tvalid_0's l2: 0.861692\n",
      "[341]\tvalid_0's l2: 0.861693\n",
      "[342]\tvalid_0's l2: 0.861679\n",
      "[343]\tvalid_0's l2: 0.861679\n",
      "[344]\tvalid_0's l2: 0.861679\n",
      "[345]\tvalid_0's l2: 0.86167\n",
      "[346]\tvalid_0's l2: 0.861656\n",
      "[347]\tvalid_0's l2: 0.861649\n",
      "[348]\tvalid_0's l2: 0.86164\n",
      "[349]\tvalid_0's l2: 0.861626\n",
      "[350]\tvalid_0's l2: 0.861615\n",
      "[351]\tvalid_0's l2: 0.861613\n",
      "[352]\tvalid_0's l2: 0.861607\n",
      "[353]\tvalid_0's l2: 0.861587\n",
      "[354]\tvalid_0's l2: 0.861579\n",
      "[355]\tvalid_0's l2: 0.861562\n",
      "[356]\tvalid_0's l2: 0.861541\n",
      "[357]\tvalid_0's l2: 0.861541\n",
      "[358]\tvalid_0's l2: 0.861518\n",
      "[359]\tvalid_0's l2: 0.861516\n",
      "[360]\tvalid_0's l2: 0.861516\n",
      "[361]\tvalid_0's l2: 0.86152\n",
      "[362]\tvalid_0's l2: 0.861504\n",
      "[363]\tvalid_0's l2: 0.861518\n",
      "[364]\tvalid_0's l2: 0.861511\n",
      "[365]\tvalid_0's l2: 0.861507\n",
      "[366]\tvalid_0's l2: 0.861498\n",
      "[367]\tvalid_0's l2: 0.861504\n",
      "[368]\tvalid_0's l2: 0.861493\n",
      "[369]\tvalid_0's l2: 0.861488\n",
      "[370]\tvalid_0's l2: 0.861477\n",
      "[371]\tvalid_0's l2: 0.861481\n",
      "[372]\tvalid_0's l2: 0.861473\n",
      "[373]\tvalid_0's l2: 0.861482\n",
      "[374]\tvalid_0's l2: 0.861466\n",
      "[375]\tvalid_0's l2: 0.861459\n",
      "[376]\tvalid_0's l2: 0.861459\n",
      "[377]\tvalid_0's l2: 0.861431\n",
      "[378]\tvalid_0's l2: 0.861404\n",
      "[379]\tvalid_0's l2: 0.861385\n",
      "[380]\tvalid_0's l2: 0.861383\n",
      "[381]\tvalid_0's l2: 0.861395\n",
      "[382]\tvalid_0's l2: 0.861392\n",
      "[383]\tvalid_0's l2: 0.861402\n",
      "[384]\tvalid_0's l2: 0.86138\n",
      "[385]\tvalid_0's l2: 0.861351\n",
      "[386]\tvalid_0's l2: 0.861338\n",
      "[387]\tvalid_0's l2: 0.861342\n",
      "[388]\tvalid_0's l2: 0.861326\n",
      "[389]\tvalid_0's l2: 0.861307\n",
      "[390]\tvalid_0's l2: 0.861311\n",
      "[391]\tvalid_0's l2: 0.861305\n",
      "[392]\tvalid_0's l2: 0.861304\n",
      "[393]\tvalid_0's l2: 0.861297\n",
      "[394]\tvalid_0's l2: 0.86128\n",
      "[395]\tvalid_0's l2: 0.861272\n",
      "[396]\tvalid_0's l2: 0.861279\n",
      "[397]\tvalid_0's l2: 0.861265\n",
      "[398]\tvalid_0's l2: 0.861266\n",
      "[399]\tvalid_0's l2: 0.861264\n",
      "[400]\tvalid_0's l2: 0.861266\n",
      "[401]\tvalid_0's l2: 0.861271\n",
      "[402]\tvalid_0's l2: 0.861258\n",
      "[403]\tvalid_0's l2: 0.861263\n",
      "[404]\tvalid_0's l2: 0.86128\n",
      "[405]\tvalid_0's l2: 0.861264\n",
      "[406]\tvalid_0's l2: 0.861265\n",
      "[407]\tvalid_0's l2: 0.861257\n",
      "[408]\tvalid_0's l2: 0.86126\n",
      "[409]\tvalid_0's l2: 0.861257\n",
      "[410]\tvalid_0's l2: 0.861231\n",
      "[411]\tvalid_0's l2: 0.861231\n",
      "[412]\tvalid_0's l2: 0.861219\n",
      "[413]\tvalid_0's l2: 0.861207\n",
      "[414]\tvalid_0's l2: 0.861211\n",
      "[415]\tvalid_0's l2: 0.861211\n",
      "[416]\tvalid_0's l2: 0.861195\n",
      "[417]\tvalid_0's l2: 0.861194\n",
      "[418]\tvalid_0's l2: 0.861183\n",
      "[419]\tvalid_0's l2: 0.861161\n",
      "[420]\tvalid_0's l2: 0.861137\n",
      "[421]\tvalid_0's l2: 0.861143\n",
      "[422]\tvalid_0's l2: 0.861136\n",
      "[423]\tvalid_0's l2: 0.86114\n",
      "[424]\tvalid_0's l2: 0.861146\n",
      "[425]\tvalid_0's l2: 0.861151\n",
      "[426]\tvalid_0's l2: 0.861156\n",
      "[427]\tvalid_0's l2: 0.861169\n",
      "[428]\tvalid_0's l2: 0.861154\n",
      "[429]\tvalid_0's l2: 0.861144\n",
      "[430]\tvalid_0's l2: 0.861137\n",
      "[431]\tvalid_0's l2: 0.861116\n",
      "[432]\tvalid_0's l2: 0.861136\n",
      "[433]\tvalid_0's l2: 0.861124\n",
      "[434]\tvalid_0's l2: 0.861118\n",
      "[435]\tvalid_0's l2: 0.861119\n",
      "[436]\tvalid_0's l2: 0.861108\n",
      "[437]\tvalid_0's l2: 0.861086\n",
      "[438]\tvalid_0's l2: 0.861107\n",
      "[439]\tvalid_0's l2: 0.861091\n",
      "[440]\tvalid_0's l2: 0.861085\n",
      "[441]\tvalid_0's l2: 0.861068\n",
      "[442]\tvalid_0's l2: 0.861055\n",
      "[443]\tvalid_0's l2: 0.861034\n",
      "[444]\tvalid_0's l2: 0.861059\n",
      "[445]\tvalid_0's l2: 0.861044\n",
      "[446]\tvalid_0's l2: 0.861037\n",
      "[447]\tvalid_0's l2: 0.861027\n",
      "[448]\tvalid_0's l2: 0.861013\n",
      "[449]\tvalid_0's l2: 0.860994\n",
      "[450]\tvalid_0's l2: 0.860983\n",
      "[451]\tvalid_0's l2: 0.860968\n",
      "[452]\tvalid_0's l2: 0.860973\n",
      "[453]\tvalid_0's l2: 0.860962\n",
      "[454]\tvalid_0's l2: 0.860951\n",
      "[455]\tvalid_0's l2: 0.860963\n",
      "[456]\tvalid_0's l2: 0.860953\n",
      "[457]\tvalid_0's l2: 0.860964\n",
      "[458]\tvalid_0's l2: 0.860977\n",
      "[459]\tvalid_0's l2: 0.860969\n",
      "[460]\tvalid_0's l2: 0.860963\n",
      "[461]\tvalid_0's l2: 0.860967\n",
      "[462]\tvalid_0's l2: 0.860956\n",
      "[463]\tvalid_0's l2: 0.860948\n",
      "[464]\tvalid_0's l2: 0.860942\n",
      "[465]\tvalid_0's l2: 0.86095\n",
      "[466]\tvalid_0's l2: 0.860952\n",
      "[467]\tvalid_0's l2: 0.860947\n",
      "[468]\tvalid_0's l2: 0.860958\n",
      "[469]\tvalid_0's l2: 0.860947\n",
      "[470]\tvalid_0's l2: 0.860924\n",
      "[471]\tvalid_0's l2: 0.860905\n",
      "[472]\tvalid_0's l2: 0.860897\n",
      "[473]\tvalid_0's l2: 0.860909\n",
      "[474]\tvalid_0's l2: 0.860911\n",
      "[475]\tvalid_0's l2: 0.86091\n",
      "[476]\tvalid_0's l2: 0.86092\n",
      "[477]\tvalid_0's l2: 0.860921\n",
      "[478]\tvalid_0's l2: 0.860916\n",
      "[479]\tvalid_0's l2: 0.860921\n",
      "[480]\tvalid_0's l2: 0.860908\n",
      "[481]\tvalid_0's l2: 0.860892\n",
      "[482]\tvalid_0's l2: 0.86089\n",
      "[483]\tvalid_0's l2: 0.860894\n",
      "[484]\tvalid_0's l2: 0.86089\n",
      "[485]\tvalid_0's l2: 0.860891\n",
      "[486]\tvalid_0's l2: 0.860897\n",
      "[487]\tvalid_0's l2: 0.860914\n",
      "[488]\tvalid_0's l2: 0.86091\n",
      "[489]\tvalid_0's l2: 0.860911\n",
      "[490]\tvalid_0's l2: 0.860904\n",
      "[491]\tvalid_0's l2: 0.860906\n",
      "[492]\tvalid_0's l2: 0.860911\n",
      "Early stopping, best iteration is:\n",
      "[482]\tvalid_0's l2: 0.86089\n",
      "Validation Pearsonr score : 0.1167\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n",
    "params = {'seed': 1,\n",
    "           'objective': \"regression\",\n",
    "           'learning_rate': 0.02,\n",
    "           'bagging_fraction': 0.2,\n",
    "           'bagging_freq': 1,\n",
    "           'feature_fraction': 0.3,\n",
    "           'max_depth': 10,\n",
    "           'min_child_samples': 50,\n",
    "           'num_leaves': 64}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=19450815,\n",
    "                valid_sets=lgb_eval,\n",
    "                #verbose_eval=False,\n",
    "                early_stopping_rounds=10,\n",
    "                )\n",
    "\n",
    "Y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "score_tuple = pearsonr(Y_test, Y_pred)\n",
    "score = score_tuple[0]\n",
    "print(f\"Validation Pearsonr score : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1167423867739778, 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   f_231   23532.11968612671\n",
      "2   f_78   13706.843276977539\n",
      "3   f_179   10180.257566452026\n",
      "4   f_21   9806.823657989502\n",
      "5   f_153   9600.540709495544\n",
      "6   f_145   9136.982006072998\n",
      "7   f_174   7784.722521781921\n",
      "8   f_225   7580.068103790283\n",
      "9   f_243   6584.840175628662\n",
      "10   f_63   6419.694696426392\n",
      "11   diff_average   6240.60942363739\n",
      "12   f_221   6085.228004455566\n",
      "13   f_232   5853.3870677948\n",
      "14   f_196   5650.753523826599\n",
      "15   f_118   5552.661417007446\n",
      "16   f_204   5546.735206604004\n",
      "17   f_119   5474.612819671631\n",
      "18   f_292   5423.446388244629\n",
      "19   f_278   5401.3536014556885\n",
      "20   f_240   5368.9991092681885\n",
      "21   f_146   5296.248898506165\n",
      "22   f_74   5188.107501983643\n",
      "23   f_65   5077.258892059326\n",
      "24   f_165   5027.257196426392\n",
      "25   f_125   5017.205196380615\n",
      "26   f_286   5002.036408424377\n",
      "27   f_8   4986.76628112793\n",
      "28   f_255   4948.466981887817\n",
      "29   f_155   4936.950271606445\n",
      "30   f_58   4923.749111175537\n",
      "31   f_247   4775.036600112915\n",
      "32   f_295   4655.552589416504\n",
      "33   f_19   4650.888790130615\n",
      "34   f_62   4644.977705001831\n",
      "35   f_130   4588.895879745483\n",
      "36   f_241   4518.888597488403\n",
      "37   f_183   4501.879304885864\n",
      "38   f_193   4488.419603347778\n",
      "39   f_59   4488.314502716064\n",
      "40   f_250   4460.8528871536255\n",
      "41   f_197   4439.7350034713745\n",
      "42   f_123   4373.446590423584\n",
      "43   f_73   4366.250088691711\n",
      "44   f_168   4316.585311889648\n",
      "45   f_114   4200.178388595581\n",
      "46   f_25   4155.868701934814\n",
      "47   f_108   4151.185899734497\n",
      "48   f_178   4133.223405838013\n",
      "49   f_212   4125.473518371582\n",
      "50   f_181   4122.949384689331\n",
      "51   f_219   4103.788508415222\n",
      "52   f_32   4101.670305252075\n",
      "53   f_71   4054.6969108581543\n",
      "54   f_41   4041.8952102661133\n",
      "55   f_29   4037.4725017547607\n",
      "56   f_61   3998.94388961792\n",
      "57   f_269   3916.2149963378906\n",
      "58   f_268   3914.381685256958\n",
      "59   f_128   3898.6577892303467\n",
      "60   f_267   3870.019193649292\n",
      "61   f_157   3858.6468048095703\n",
      "62   f_256   3828.73517036438\n",
      "63   f_138   3796.45529460907\n",
      "64   f_257   3774.3545932769775\n",
      "65   f_142   3732.583600997925\n",
      "66   f_14   3694.812915802002\n",
      "67   f_156   3679.257801055908\n",
      "68   f_122   3621.7144050598145\n",
      "69   f_100   3607.068691253662\n",
      "70   f_22   3589.621105194092\n",
      "71   f_162   3581.952501296997\n",
      "72   f_101   3566.880308151245\n",
      "73   f_127   3553.9605960845947\n",
      "74   f_214   3531.631109237671\n",
      "75   f_107   3520.322401046753\n",
      "76   f_184   3516.0783910751343\n",
      "77   f_176   3512.714401245117\n",
      "78   f_198   3510.3197917938232\n",
      "79   f_294   3501.0613117218018\n",
      "80   f_280   3478.8699045181274\n",
      "81   f_99   3478.311308860779\n",
      "82   f_209   3471.0679988861084\n",
      "83   f_69   3467.7570056915283\n",
      "84   f_52   3434.4543113708496\n",
      "85   f_3   3383.7357063293457\n",
      "86   f_223   3378.0233974456787\n",
      "87   f_56   3351.381883621216\n",
      "88   f_188   3333.2546062469482\n",
      "89   f_98   3312.9793014526367\n",
      "90   f_67   3303.71609210968\n",
      "91   f_132   3299.5635833740234\n",
      "92   f_244   3293.100803375244\n",
      "93   f_265   3285.4774990081787\n",
      "94   f_263   3279.2595958709717\n",
      "95   f_281   3275.0738954544067\n",
      "96   f_136   3254.452386856079\n",
      "97   f_171   3213.4027976989746\n",
      "98   f_94   3183.7427883148193\n",
      "99   f_237   3171.925100326538\n",
      "100   f_44   3170.798804283142\n",
      "101   f_120   3158.978307723999\n",
      "102   f_270   3143.236183166504\n",
      "103   f_42   3141.3267974853516\n",
      "104   f_16   3134.135498046875\n",
      "105   f_187   3133.506597518921\n",
      "106   f_271   3109.161506652832\n",
      "107   f_233   3105.310489654541\n",
      "108   f_185   3095.973494529724\n",
      "109   f_213   3094.144006729126\n",
      "110   f_160   3083.9299850463867\n",
      "111   f_251   3079.815420150757\n",
      "112   f_5   3078.1517963409424\n",
      "113   f_172   3070.517406463623\n",
      "114   f_110   3062.3652992248535\n",
      "115   f_207   3055.1972160339355\n",
      "116   f_150   3037.440196990967\n",
      "117   f_202   3028.988494873047\n",
      "118   f_287   3014.698595046997\n",
      "119   f_163   2996.0850133895874\n",
      "120   f_210   2988.676488876343\n",
      "121   f_26   2988.359691619873\n",
      "122   f_121   2982.8752059936523\n",
      "123   f_11   2979.288496017456\n",
      "124   f_104   2966.9257164001465\n",
      "125   f_137   2952.0821046829224\n",
      "126   f_297   2933.077178001404\n",
      "127   f_117   2926.242992401123\n",
      "128   f_68   2921.5447969436646\n",
      "129   f_51   2919.4358043670654\n",
      "130   f_289   2908.2138061523438\n",
      "131   f_252   2904.104501724243\n",
      "132   f_34   2902.097993850708\n",
      "133   f_48   2899.8744106292725\n",
      "134   f_248   2898.021797180176\n",
      "135   f_102   2883.9230937957764\n",
      "136   f_200   2846.743700027466\n",
      "137   f_81   2843.680311203003\n",
      "138   f_116   2839.611898422241\n",
      "139   f_147   2822.48189163208\n",
      "140   f_83   2816.6867904663086\n",
      "141   f_134   2808.8284044265747\n",
      "142   f_169   2795.870407104492\n",
      "143   f_235   2795.6316089630127\n",
      "144   f_2   2785.931692123413\n",
      "145   f_177   2781.8116064071655\n",
      "146   f_151   2781.30979347229\n",
      "147   f_208   2767.9692993164062\n",
      "148   f_149   2750.286388397217\n",
      "149   f_79   2742.5328979492188\n",
      "150   f_31   2741.809009552002\n",
      "151   f_84   2739.950701713562\n",
      "152   f_139   2739.652105331421\n",
      "153   f_264   2737.8974952697754\n",
      "154   f_226   2736.78879737854\n",
      "155   f_249   2709.8348064422607\n",
      "156   f_293   2707.7061920166016\n",
      "157   f_28   2692.9963970184326\n",
      "158   f_274   2672.102493286133\n",
      "159   f_80   2667.091607093811\n",
      "160   f_164   2628.914701461792\n",
      "161   f_273   2628.5880069732666\n",
      "162   f_109   2612.3207969665527\n",
      "163   f_229   2592.1324043273926\n",
      "164   f_1   2589.1083068847656\n",
      "165   f_220   2581.969493865967\n",
      "166   f_275   2563.8185024261475\n",
      "167   f_93   2563.3233966827393\n",
      "168   f_283   2562.3891925811768\n",
      "169   f_53   2557.24760055542\n",
      "170   f_238   2556.4361000061035\n",
      "171   f_246   2529.0285987854004\n",
      "172   f_161   2524.051597595215\n",
      "173   f_254   2521.821195602417\n",
      "174   f_199   2501.2299995422363\n",
      "175   f_15   2499.55011177063\n",
      "176   f_192   2495.890613555908\n",
      "177   f_218   2491.6871995925903\n",
      "178   f_76   2473.9816942214966\n",
      "179   f_27   2472.733094215393\n",
      "180   f_282   2459.4174995422363\n",
      "181   f_227   2452.8562965393066\n",
      "182   f_18   2447.1530170440674\n",
      "183   f_36   2444.8247985839844\n",
      "184   f_57   2438.371406555176\n",
      "185   f_291   2426.8739051818848\n",
      "186   f_216   2425.429605484009\n",
      "187   f_190   2424.427698135376\n",
      "188   f_131   2418.7573051452637\n",
      "189   f_0   2409.846004486084\n",
      "190   f_88   2386.458402633667\n",
      "191   f_7   2385.940999984741\n",
      "192   f_224   2385.7566051483154\n",
      "193   f_10   2375.631904602051\n",
      "194   f_33   2368.2175998687744\n",
      "195   f_194   2365.920701980591\n",
      "196   f_133   2360.404001235962\n",
      "197   f_234   2359.0323944091797\n",
      "198   f_144   2355.7611045837402\n",
      "199   f_180   2327.550994873047\n",
      "200   f_135   2327.0760011672974\n",
      "201   f_90   2316.56760597229\n",
      "202   f_82   2296.964593887329\n",
      "203   f_288   2288.5368041992188\n",
      "204   f_105   2285.9540996551514\n",
      "205   f_261   2266.2088108062744\n",
      "206   f_242   2259.1670112609863\n",
      "207   f_23   2251.6133975982666\n",
      "208   f_64   2244.984501838684\n",
      "209   f_75   2239.5645999908447\n",
      "210   f_152   2219.5337085723877\n",
      "211   f_54   2203.205602645874\n",
      "212   f_37   2194.9703063964844\n",
      "213   f_92   2189.8676948547363\n",
      "214   f_276   2186.7243976593018\n",
      "215   f_159   2153.645502090454\n",
      "216   f_195   2129.2776012420654\n",
      "217   f_277   2127.2177019119263\n",
      "218   f_106   2126.7985038757324\n",
      "219   f_70   2117.0921058654785\n",
      "220   f_60   2110.099394798279\n",
      "221   f_39   2107.205009460449\n",
      "222   f_215   2102.436403274536\n",
      "223   f_230   2100.3932971954346\n",
      "224   f_201   2098.1131076812744\n",
      "225   f_284   2089.807201385498\n",
      "226   f_6   2074.2087955474854\n",
      "227   f_140   2054.4323024749756\n",
      "228   f_143   2051.932996749878\n",
      "229   f_91   2049.244607925415\n",
      "230   f_113   2042.614101409912\n",
      "231   f_170   2031.7741012573242\n",
      "232   f_285   2021.5476989746094\n",
      "233   f_239   2021.0753946304321\n",
      "234   f_260   2017.419101715088\n",
      "235   f_203   2004.7045040130615\n",
      "236   f_85   1998.1652946472168\n",
      "237   f_112   1955.3105010986328\n",
      "238   f_96   1938.6500034332275\n",
      "239   f_290   1936.9692974090576\n",
      "240   f_167   1927.5982055664062\n",
      "241   f_158   1918.3991012573242\n",
      "242   f_89   1910.5797023773193\n",
      "243   f_77   1900.7525939941406\n",
      "244   f_55   1894.6463985443115\n",
      "245   f_47   1877.1195993423462\n",
      "246   f_13   1849.7141952514648\n",
      "247   f_12   1840.5837955474854\n",
      "248   f_30   1834.3964958190918\n",
      "249   f_186   1827.662799835205\n",
      "250   f_189   1817.908302307129\n",
      "251   f_299   1811.6195030212402\n",
      "252   f_72   1803.949104309082\n",
      "253   f_262   1774.3262977600098\n",
      "254   f_38   1774.1108932495117\n",
      "255   f_296   1770.2200965881348\n",
      "256   f_279   1749.331997871399\n",
      "257   f_87   1742.0078029632568\n",
      "258   f_141   1740.141399383545\n",
      "259   f_46   1737.810606956482\n",
      "260   f_259   1736.3507051467896\n",
      "261   f_103   1715.6645030975342\n",
      "262   f_236   1711.7835006713867\n",
      "263   f_175   1703.8881034851074\n",
      "264   f_17   1690.691987991333\n",
      "265   f_206   1685.400001525879\n",
      "266   f_45   1683.8032989501953\n",
      "267   f_154   1682.6163959503174\n",
      "268   f_126   1679.6191959381104\n",
      "269   f_182   1673.6370944976807\n",
      "270   f_129   1660.7417964935303\n",
      "271   f_166   1659.0001049041748\n",
      "272   f_173   1641.3655090332031\n",
      "273   f_298   1633.5925998687744\n",
      "274   f_50   1631.795199394226\n",
      "275   f_66   1584.6809005737305\n",
      "276   f_245   1573.6475067138672\n",
      "277   f_95   1572.0105953216553\n",
      "278   f_115   1566.3445014953613\n",
      "279   f_24   1549.3421077728271\n",
      "280   f_222   1544.8770065307617\n",
      "281   f_217   1527.632791519165\n",
      "282   f_253   1498.2789993286133\n",
      "283   f_266   1496.679401397705\n",
      "284   f_148   1476.1157035827637\n",
      "285   f_9   1457.4390983581543\n",
      "286   f_35   1449.722993850708\n",
      "287   f_191   1435.094898223877\n",
      "288   f_86   1428.211498260498\n",
      "289   f_272   1417.8187017440796\n",
      "290   f_20   1380.7164974212646\n",
      "291   f_43   1380.255895614624\n",
      "292   f_211   1365.9697036743164\n",
      "293   f_40   1338.7280006408691\n",
      "294   f_258   1323.0303993225098\n",
      "295   f_205   1302.5867977142334\n",
      "296   f_97   1292.444492340088\n",
      "297   f_111   1255.106496810913\n",
      "298   f_228   1252.6910009384155\n",
      "299   f_4   1236.6391048431396\n",
      "300   f_124   996.4143943786621\n",
      "301   f_49   863.4412021636963\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "feature = gbm.feature_importance(importance_type='gain')\n",
    "\n",
    "#\n",
    "f = pd.DataFrame({'number': range(0, len(feature)),\n",
    "             'feature': feature[:]})\n",
    "f2 = f.sort_values('feature',ascending=False)\n",
    "\n",
    "#\n",
    "label = X_train.columns[0:]\n",
    "\n",
    "#\n",
    "indices = np.argsort(feature)[::-1]\n",
    "\n",
    "for i in range(len(feature)):\n",
    "    print(str(i + 1) + \"   \" + str(label[indices[i]]) + \"   \" + str(feature[indices[i]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(len(feature)),feature[indices], color='lightblue', align='center')\n",
    "plt.xticks(range(len(feature)), label[indices], rotation = 90)\n",
    "plt.xlim([-1, len(feature)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>...</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "      <th>diff_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.300781</td>\n",
       "      <td>0.932617</td>\n",
       "      <td>0.113708</td>\n",
       "      <td>-0.402100</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>-0.203979</td>\n",
       "      <td>-0.413574</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095703</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>0.819336</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.086792</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-1.044922</td>\n",
       "      <td>-0.287598</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.231079</td>\n",
       "      <td>0.811035</td>\n",
       "      <td>-0.514160</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>-0.616699</td>\n",
       "      <td>-0.194214</td>\n",
       "      <td>1.771484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.734375</td>\n",
       "      <td>0.819336</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.387695</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-0.929688</td>\n",
       "      <td>-0.974121</td>\n",
       "      <td>-0.343506</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>0.394043</td>\n",
       "      <td>0.615723</td>\n",
       "      <td>0.567871</td>\n",
       "      <td>-0.607910</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>-1.083008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.551758</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>-1.060547</td>\n",
       "      <td>-0.219116</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-0.612305</td>\n",
       "      <td>-0.113953</td>\n",
       "      <td>0.243652</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.064453</td>\n",
       "      <td>-2.343750</td>\n",
       "      <td>-0.011871</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>-0.606445</td>\n",
       "      <td>-0.586914</td>\n",
       "      <td>-0.815918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.266357</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.608887</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>-0.783203</td>\n",
       "      <td>1.151367</td>\n",
       "      <td>-0.773438</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.531738</td>\n",
       "      <td>0.842285</td>\n",
       "      <td>-0.262939</td>\n",
       "      <td>2.330078</td>\n",
       "      <td>-0.583496</td>\n",
       "      <td>-0.618164</td>\n",
       "      <td>-0.742676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.741211</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.588379</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>0.753418</td>\n",
       "      <td>1.345703</td>\n",
       "      <td>-0.737793</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141405</th>\n",
       "      <td>12193768.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.093506</td>\n",
       "      <td>-0.720215</td>\n",
       "      <td>-0.345459</td>\n",
       "      <td>-0.438721</td>\n",
       "      <td>-0.166992</td>\n",
       "      <td>-0.437256</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.232422</td>\n",
       "      <td>-0.660645</td>\n",
       "      <td>0.875488</td>\n",
       "      <td>0.421631</td>\n",
       "      <td>-0.427979</td>\n",
       "      <td>-0.075562</td>\n",
       "      <td>-0.533203</td>\n",
       "      <td>-0.193726</td>\n",
       "      <td>-0.581543</td>\n",
       "      <td>0.019530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141406</th>\n",
       "      <td>12193769.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>3769.0</td>\n",
       "      <td>-0.223267</td>\n",
       "      <td>-1.344727</td>\n",
       "      <td>-0.199951</td>\n",
       "      <td>-0.107727</td>\n",
       "      <td>-0.454590</td>\n",
       "      <td>-0.221924</td>\n",
       "      <td>-0.141113</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.232422</td>\n",
       "      <td>-0.670410</td>\n",
       "      <td>0.875488</td>\n",
       "      <td>0.421631</td>\n",
       "      <td>-0.729980</td>\n",
       "      <td>-1.514648</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>-0.890137</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>-0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141407</th>\n",
       "      <td>12193770.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>3770.0</td>\n",
       "      <td>-0.559570</td>\n",
       "      <td>0.979492</td>\n",
       "      <td>-1.110352</td>\n",
       "      <td>1.006836</td>\n",
       "      <td>-0.467285</td>\n",
       "      <td>-0.159546</td>\n",
       "      <td>1.355469</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.232422</td>\n",
       "      <td>0.820801</td>\n",
       "      <td>-1.142578</td>\n",
       "      <td>0.421631</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>1.363281</td>\n",
       "      <td>-0.079102</td>\n",
       "      <td>-1.580078</td>\n",
       "      <td>-0.297607</td>\n",
       "      <td>-0.033578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141408</th>\n",
       "      <td>12193772.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>3772.0</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>-2.564453</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.076599</td>\n",
       "      <td>1.379883</td>\n",
       "      <td>-0.155396</td>\n",
       "      <td>-0.688965</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.232422</td>\n",
       "      <td>0.133057</td>\n",
       "      <td>-1.142578</td>\n",
       "      <td>0.421631</td>\n",
       "      <td>-0.375244</td>\n",
       "      <td>-1.514648</td>\n",
       "      <td>-0.973633</td>\n",
       "      <td>0.608887</td>\n",
       "      <td>-0.372070</td>\n",
       "      <td>-0.036222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141409</th>\n",
       "      <td>12193773.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>3773.0</td>\n",
       "      <td>1.211914</td>\n",
       "      <td>-0.089539</td>\n",
       "      <td>0.190186</td>\n",
       "      <td>-0.548340</td>\n",
       "      <td>0.151245</td>\n",
       "      <td>0.079773</td>\n",
       "      <td>0.447998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811523</td>\n",
       "      <td>3.271484</td>\n",
       "      <td>0.875488</td>\n",
       "      <td>0.421631</td>\n",
       "      <td>-0.170654</td>\n",
       "      <td>1.363281</td>\n",
       "      <td>-0.563477</td>\n",
       "      <td>0.669434</td>\n",
       "      <td>0.456299</td>\n",
       "      <td>0.085413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141410 rows  305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             row_id  time_id  investment_id    target       f_0       f_1  \\\n",
       "0               1.0      0.0            1.0 -0.300781  0.932617  0.113708   \n",
       "1               2.0      0.0            2.0 -0.231079  0.811035 -0.514160   \n",
       "2               6.0      0.0            6.0  0.568848  0.394043  0.615723   \n",
       "3               7.0      0.0            7.0 -1.064453 -2.343750 -0.011871   \n",
       "4               8.0      0.0            8.0 -0.531738  0.842285 -0.262939   \n",
       "...             ...      ...            ...       ...       ...       ...   \n",
       "3141405  12193768.0   1219.0         3768.0  0.033600  0.093506 -0.720215   \n",
       "3141406  12193769.0   1219.0         3769.0 -0.223267 -1.344727 -0.199951   \n",
       "3141407  12193770.0   1219.0         3770.0 -0.559570  0.979492 -1.110352   \n",
       "3141408  12193772.0   1219.0         3772.0  0.009598 -2.564453  0.320312   \n",
       "3141409  12193773.0   1219.0         3773.0  1.211914 -0.089539  0.190186   \n",
       "\n",
       "              f_2       f_3       f_4       f_5  ...     f_291     f_292  \\\n",
       "0       -0.402100  0.378418 -0.203979 -0.413574  ... -1.095703  0.200073   \n",
       "1        0.742188 -0.616699 -0.194214  1.771484  ...  0.912598 -0.734375   \n",
       "2        0.567871 -0.607910  0.068909 -1.083008  ...  0.912598 -0.551758   \n",
       "3        1.875000 -0.606445 -0.586914 -0.815918  ...  0.912598 -0.266357   \n",
       "4        2.330078 -0.583496 -0.618164 -0.742676  ...  0.912598 -0.741211   \n",
       "...           ...       ...       ...       ...  ...       ...       ...   \n",
       "3141405 -0.345459 -0.438721 -0.166992 -0.437256  ... -1.232422 -0.660645   \n",
       "3141406 -0.107727 -0.454590 -0.221924 -0.141113  ... -1.232422 -0.670410   \n",
       "3141407  1.006836 -0.467285 -0.159546  1.355469  ... -1.232422  0.820801   \n",
       "3141408  0.076599  1.379883 -0.155396 -0.688965  ... -1.232422  0.133057   \n",
       "3141409 -0.548340  0.151245  0.079773  0.447998  ...  0.811523  3.271484   \n",
       "\n",
       "            f_293     f_294     f_295     f_296     f_297     f_298     f_299  \\\n",
       "0        0.819336  0.941406 -0.086792 -1.086914 -1.044922 -0.287598  0.321533   \n",
       "1        0.819336  0.941406 -0.387695 -1.086914 -0.929688 -0.974121 -0.343506   \n",
       "2       -1.220703 -1.060547 -0.219116 -1.086914 -0.612305 -0.113953  0.243652   \n",
       "3       -1.220703  0.941406 -0.608887  0.104919 -0.783203  1.151367 -0.773438   \n",
       "4       -1.220703  0.941406 -0.588379  0.104919  0.753418  1.345703 -0.737793   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "3141405  0.875488  0.421631 -0.427979 -0.075562 -0.533203 -0.193726 -0.581543   \n",
       "3141406  0.875488  0.421631 -0.729980 -1.514648  0.013145 -0.890137 -0.589844   \n",
       "3141407 -1.142578  0.421631 -0.363281  1.363281 -0.079102 -1.580078 -0.297607   \n",
       "3141408 -1.142578  0.421631 -0.375244 -1.514648 -0.973633  0.608887 -0.372070   \n",
       "3141409  0.875488  0.421631 -0.170654  1.363281 -0.563477  0.669434  0.456299   \n",
       "\n",
       "         diff_average  \n",
       "0            0.000000  \n",
       "1            0.000000  \n",
       "2            0.000000  \n",
       "3            0.000000  \n",
       "4            0.000000  \n",
       "...               ...  \n",
       "3141405      0.019530  \n",
       "3141406     -0.001459  \n",
       "3141407     -0.033578  \n",
       "3141408     -0.036222  \n",
       "3141409      0.085413  \n",
       "\n",
       "[3141410 rows x 305 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "C = 1.\n",
    "kernel = 'rbf'\n",
    "gamma  = 0.01\n",
    "estimator = SVC(C=C, kernel=kernel, gamma=gamma)\n",
    "classifier = OneVsRestClassifier(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path2 = '/home/ryo/Work/Python_Work/kaggle/ubiquent_martket/train_low_memory_diffAve.parquet'\n",
    "df3 = pd.read_parquet(out_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path2 = '/home/ryo/Work/Python_Work/kaggle/ubiquent_martket/train_low_memory_diffAve.parquet'\n",
    "df3 = pd.read_parquet(out_path2)\n",
    "features = [f'f_{i}' for i in range(300)]\n",
    "features.append('diff_average')\n",
    "target = 'target'\n",
    "df_features = df3[features]\n",
    "X_train, X, Y_train, Y = train_test_split(df_features, df3[target], train_size=0.6, shuffle=False)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X, Y, train_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thundersvm import SVC\n",
    "C = 1.\n",
    "kernel = 'rbf'\n",
    "model = SVC(C=C, kernel=kernel)\n",
    "# model.fit(X_train.head(1000), (Y_train.head(1000).values))\n",
    "model.fit(X_train.head(1000), (Y_train.head(1000).values*5).astype('int')/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train.head(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.016878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76755\n",
      "[LightGBM] [Info] Number of data points in the train set: 1884846, number of used features: 301\n",
      "[LightGBM] [Info] Start training from score -0.027864\n",
      "[1]\tvalid_0's l2: 0.873645\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's l2: 0.873434\n",
      "[3]\tvalid_0's l2: 0.873199\n",
      "[4]\tvalid_0's l2: 0.872981\n",
      "[5]\tvalid_0's l2: 0.872766\n",
      "[6]\tvalid_0's l2: 0.872569\n",
      "[7]\tvalid_0's l2: 0.872381\n",
      "[8]\tvalid_0's l2: 0.872209\n",
      "[9]\tvalid_0's l2: 0.872021\n",
      "[10]\tvalid_0's l2: 0.871825\n",
      "[11]\tvalid_0's l2: 0.871663\n",
      "[12]\tvalid_0's l2: 0.871506\n",
      "[13]\tvalid_0's l2: 0.871355\n",
      "[14]\tvalid_0's l2: 0.871202\n",
      "[15]\tvalid_0's l2: 0.871045\n",
      "[16]\tvalid_0's l2: 0.87089\n",
      "[17]\tvalid_0's l2: 0.870773\n",
      "[18]\tvalid_0's l2: 0.870636\n",
      "[19]\tvalid_0's l2: 0.870513\n",
      "[20]\tvalid_0's l2: 0.870395\n",
      "[21]\tvalid_0's l2: 0.87026\n",
      "[22]\tvalid_0's l2: 0.870142\n",
      "[23]\tvalid_0's l2: 0.870026\n",
      "[24]\tvalid_0's l2: 0.869915\n",
      "[25]\tvalid_0's l2: 0.869799\n",
      "[26]\tvalid_0's l2: 0.8697\n",
      "[27]\tvalid_0's l2: 0.8696\n",
      "[28]\tvalid_0's l2: 0.869484\n",
      "[29]\tvalid_0's l2: 0.869389\n",
      "[30]\tvalid_0's l2: 0.869292\n",
      "[31]\tvalid_0's l2: 0.869215\n",
      "[32]\tvalid_0's l2: 0.869116\n",
      "[33]\tvalid_0's l2: 0.869033\n",
      "[34]\tvalid_0's l2: 0.868949\n",
      "[35]\tvalid_0's l2: 0.868856\n",
      "[36]\tvalid_0's l2: 0.868769\n",
      "[37]\tvalid_0's l2: 0.8687\n",
      "[38]\tvalid_0's l2: 0.868605\n",
      "[39]\tvalid_0's l2: 0.868516\n",
      "[40]\tvalid_0's l2: 0.868442\n",
      "[41]\tvalid_0's l2: 0.868373\n",
      "[42]\tvalid_0's l2: 0.868307\n",
      "[43]\tvalid_0's l2: 0.868247\n",
      "[44]\tvalid_0's l2: 0.868186\n",
      "[45]\tvalid_0's l2: 0.868143\n",
      "[46]\tvalid_0's l2: 0.868065\n",
      "[47]\tvalid_0's l2: 0.868006\n",
      "[48]\tvalid_0's l2: 0.867943\n",
      "[49]\tvalid_0's l2: 0.867891\n",
      "[50]\tvalid_0's l2: 0.867834\n",
      "[51]\tvalid_0's l2: 0.867759\n",
      "[52]\tvalid_0's l2: 0.8677\n",
      "[53]\tvalid_0's l2: 0.867663\n",
      "[54]\tvalid_0's l2: 0.867608\n",
      "[55]\tvalid_0's l2: 0.867559\n",
      "[56]\tvalid_0's l2: 0.867505\n",
      "[57]\tvalid_0's l2: 0.867461\n",
      "[58]\tvalid_0's l2: 0.86742\n",
      "[59]\tvalid_0's l2: 0.867389\n",
      "[60]\tvalid_0's l2: 0.867329\n",
      "[61]\tvalid_0's l2: 0.867295\n",
      "[62]\tvalid_0's l2: 0.867257\n",
      "[63]\tvalid_0's l2: 0.867203\n",
      "[64]\tvalid_0's l2: 0.867176\n",
      "[65]\tvalid_0's l2: 0.867129\n",
      "[66]\tvalid_0's l2: 0.867101\n",
      "[67]\tvalid_0's l2: 0.867056\n",
      "[68]\tvalid_0's l2: 0.867016\n",
      "[69]\tvalid_0's l2: 0.866969\n",
      "[70]\tvalid_0's l2: 0.866924\n",
      "[71]\tvalid_0's l2: 0.866884\n",
      "[72]\tvalid_0's l2: 0.866831\n",
      "[73]\tvalid_0's l2: 0.866802\n",
      "[74]\tvalid_0's l2: 0.866746\n",
      "[75]\tvalid_0's l2: 0.866711\n",
      "[76]\tvalid_0's l2: 0.866667\n",
      "[77]\tvalid_0's l2: 0.866635\n",
      "[78]\tvalid_0's l2: 0.866608\n",
      "[79]\tvalid_0's l2: 0.866563\n",
      "[80]\tvalid_0's l2: 0.866535\n",
      "[81]\tvalid_0's l2: 0.866496\n",
      "[82]\tvalid_0's l2: 0.866458\n",
      "[83]\tvalid_0's l2: 0.866439\n",
      "[84]\tvalid_0's l2: 0.8664\n",
      "[85]\tvalid_0's l2: 0.86634\n",
      "[86]\tvalid_0's l2: 0.866322\n",
      "[87]\tvalid_0's l2: 0.866296\n",
      "[88]\tvalid_0's l2: 0.866268\n",
      "[89]\tvalid_0's l2: 0.866245\n",
      "[90]\tvalid_0's l2: 0.866216\n",
      "[91]\tvalid_0's l2: 0.866201\n",
      "[92]\tvalid_0's l2: 0.866182\n",
      "[93]\tvalid_0's l2: 0.866168\n",
      "[94]\tvalid_0's l2: 0.866152\n",
      "[95]\tvalid_0's l2: 0.866146\n",
      "[96]\tvalid_0's l2: 0.866119\n",
      "[97]\tvalid_0's l2: 0.866086\n",
      "[98]\tvalid_0's l2: 0.866057\n",
      "[99]\tvalid_0's l2: 0.866026\n",
      "[100]\tvalid_0's l2: 0.866008\n",
      "[101]\tvalid_0's l2: 0.86598\n",
      "[102]\tvalid_0's l2: 0.865952\n",
      "[103]\tvalid_0's l2: 0.865938\n",
      "[104]\tvalid_0's l2: 0.865903\n",
      "[105]\tvalid_0's l2: 0.865882\n",
      "[106]\tvalid_0's l2: 0.865853\n",
      "[107]\tvalid_0's l2: 0.865827\n",
      "[108]\tvalid_0's l2: 0.865794\n",
      "[109]\tvalid_0's l2: 0.865773\n",
      "[110]\tvalid_0's l2: 0.865741\n",
      "[111]\tvalid_0's l2: 0.865717\n",
      "[112]\tvalid_0's l2: 0.865682\n",
      "[113]\tvalid_0's l2: 0.865651\n",
      "[114]\tvalid_0's l2: 0.865631\n",
      "[115]\tvalid_0's l2: 0.865618\n",
      "[116]\tvalid_0's l2: 0.865586\n",
      "[117]\tvalid_0's l2: 0.865567\n",
      "[118]\tvalid_0's l2: 0.86556\n",
      "[119]\tvalid_0's l2: 0.865524\n",
      "[120]\tvalid_0's l2: 0.865492\n",
      "[121]\tvalid_0's l2: 0.865455\n",
      "[122]\tvalid_0's l2: 0.865458\n",
      "[123]\tvalid_0's l2: 0.865426\n",
      "[124]\tvalid_0's l2: 0.865392\n",
      "[125]\tvalid_0's l2: 0.865373\n",
      "[126]\tvalid_0's l2: 0.865346\n",
      "[127]\tvalid_0's l2: 0.865321\n",
      "[128]\tvalid_0's l2: 0.865296\n",
      "[129]\tvalid_0's l2: 0.865281\n",
      "[130]\tvalid_0's l2: 0.865265\n",
      "[131]\tvalid_0's l2: 0.865234\n",
      "[132]\tvalid_0's l2: 0.865214\n",
      "[133]\tvalid_0's l2: 0.865202\n",
      "[134]\tvalid_0's l2: 0.865184\n",
      "[135]\tvalid_0's l2: 0.865153\n",
      "[136]\tvalid_0's l2: 0.865146\n",
      "[137]\tvalid_0's l2: 0.865145\n",
      "[138]\tvalid_0's l2: 0.865113\n",
      "[139]\tvalid_0's l2: 0.865103\n",
      "[140]\tvalid_0's l2: 0.865083\n",
      "[141]\tvalid_0's l2: 0.865061\n",
      "[142]\tvalid_0's l2: 0.865053\n",
      "[143]\tvalid_0's l2: 0.865021\n",
      "[144]\tvalid_0's l2: 0.864993\n",
      "[145]\tvalid_0's l2: 0.864972\n",
      "[146]\tvalid_0's l2: 0.864967\n",
      "[147]\tvalid_0's l2: 0.864941\n",
      "[148]\tvalid_0's l2: 0.864918\n",
      "[149]\tvalid_0's l2: 0.864883\n",
      "[150]\tvalid_0's l2: 0.864872\n",
      "[151]\tvalid_0's l2: 0.864855\n",
      "[152]\tvalid_0's l2: 0.864842\n",
      "[153]\tvalid_0's l2: 0.864848\n",
      "[154]\tvalid_0's l2: 0.864833\n",
      "[155]\tvalid_0's l2: 0.86482\n",
      "[156]\tvalid_0's l2: 0.864818\n",
      "[157]\tvalid_0's l2: 0.864854\n",
      "[158]\tvalid_0's l2: 0.864828\n",
      "[159]\tvalid_0's l2: 0.864816\n",
      "[160]\tvalid_0's l2: 0.864786\n",
      "[161]\tvalid_0's l2: 0.86477\n",
      "[162]\tvalid_0's l2: 0.864744\n",
      "[163]\tvalid_0's l2: 0.864718\n",
      "[164]\tvalid_0's l2: 0.864703\n",
      "[165]\tvalid_0's l2: 0.864687\n",
      "[166]\tvalid_0's l2: 0.86466\n",
      "[167]\tvalid_0's l2: 0.864636\n",
      "[168]\tvalid_0's l2: 0.864607\n",
      "[169]\tvalid_0's l2: 0.864579\n",
      "[170]\tvalid_0's l2: 0.864577\n",
      "[171]\tvalid_0's l2: 0.864583\n",
      "[172]\tvalid_0's l2: 0.864595\n",
      "[173]\tvalid_0's l2: 0.86458\n",
      "[174]\tvalid_0's l2: 0.864557\n",
      "[175]\tvalid_0's l2: 0.864553\n",
      "[176]\tvalid_0's l2: 0.864528\n",
      "[177]\tvalid_0's l2: 0.864513\n",
      "[178]\tvalid_0's l2: 0.864505\n",
      "[179]\tvalid_0's l2: 0.864484\n",
      "[180]\tvalid_0's l2: 0.864472\n",
      "[181]\tvalid_0's l2: 0.864463\n",
      "[182]\tvalid_0's l2: 0.864443\n",
      "[183]\tvalid_0's l2: 0.864419\n",
      "[184]\tvalid_0's l2: 0.864393\n",
      "[185]\tvalid_0's l2: 0.864367\n",
      "[186]\tvalid_0's l2: 0.864367\n",
      "[187]\tvalid_0's l2: 0.864384\n",
      "[188]\tvalid_0's l2: 0.864387\n",
      "[189]\tvalid_0's l2: 0.864359\n",
      "[190]\tvalid_0's l2: 0.864346\n",
      "[191]\tvalid_0's l2: 0.86432\n",
      "[192]\tvalid_0's l2: 0.864307\n",
      "[193]\tvalid_0's l2: 0.864303\n",
      "[194]\tvalid_0's l2: 0.864293\n",
      "[195]\tvalid_0's l2: 0.864263\n",
      "[196]\tvalid_0's l2: 0.864256\n",
      "[197]\tvalid_0's l2: 0.864236\n",
      "[198]\tvalid_0's l2: 0.86422\n",
      "[199]\tvalid_0's l2: 0.864208\n",
      "[200]\tvalid_0's l2: 0.864186\n",
      "[201]\tvalid_0's l2: 0.864196\n",
      "[202]\tvalid_0's l2: 0.864185\n",
      "[203]\tvalid_0's l2: 0.864153\n",
      "[204]\tvalid_0's l2: 0.86415\n",
      "[205]\tvalid_0's l2: 0.864126\n",
      "[206]\tvalid_0's l2: 0.864104\n",
      "[207]\tvalid_0's l2: 0.864109\n",
      "[208]\tvalid_0's l2: 0.864087\n",
      "[209]\tvalid_0's l2: 0.864063\n",
      "[210]\tvalid_0's l2: 0.864045\n",
      "[211]\tvalid_0's l2: 0.864017\n",
      "[212]\tvalid_0's l2: 0.863986\n",
      "[213]\tvalid_0's l2: 0.863966\n",
      "[214]\tvalid_0's l2: 0.863959\n",
      "[215]\tvalid_0's l2: 0.863929\n",
      "[216]\tvalid_0's l2: 0.863896\n",
      "[217]\tvalid_0's l2: 0.86389\n",
      "[218]\tvalid_0's l2: 0.863883\n",
      "[219]\tvalid_0's l2: 0.863873\n",
      "[220]\tvalid_0's l2: 0.863873\n",
      "[221]\tvalid_0's l2: 0.863867\n",
      "[222]\tvalid_0's l2: 0.863847\n",
      "[223]\tvalid_0's l2: 0.863833\n",
      "[224]\tvalid_0's l2: 0.863815\n",
      "[225]\tvalid_0's l2: 0.863801\n",
      "[226]\tvalid_0's l2: 0.86379\n",
      "[227]\tvalid_0's l2: 0.863784\n",
      "[228]\tvalid_0's l2: 0.863772\n",
      "[229]\tvalid_0's l2: 0.863755\n",
      "[230]\tvalid_0's l2: 0.86376\n",
      "[231]\tvalid_0's l2: 0.863738\n",
      "[232]\tvalid_0's l2: 0.863707\n",
      "[233]\tvalid_0's l2: 0.863702\n",
      "[234]\tvalid_0's l2: 0.863694\n",
      "[235]\tvalid_0's l2: 0.863672\n",
      "[236]\tvalid_0's l2: 0.863669\n",
      "[237]\tvalid_0's l2: 0.863647\n",
      "[238]\tvalid_0's l2: 0.863635\n",
      "[239]\tvalid_0's l2: 0.863634\n",
      "[240]\tvalid_0's l2: 0.863629\n",
      "[241]\tvalid_0's l2: 0.863615\n",
      "[242]\tvalid_0's l2: 0.863608\n",
      "[243]\tvalid_0's l2: 0.863594\n",
      "[244]\tvalid_0's l2: 0.863574\n",
      "[245]\tvalid_0's l2: 0.863561\n",
      "[246]\tvalid_0's l2: 0.863545\n",
      "[247]\tvalid_0's l2: 0.863525\n",
      "[248]\tvalid_0's l2: 0.863505\n",
      "[249]\tvalid_0's l2: 0.863481\n",
      "[250]\tvalid_0's l2: 0.863489\n",
      "[251]\tvalid_0's l2: 0.863472\n",
      "[252]\tvalid_0's l2: 0.86347\n",
      "[253]\tvalid_0's l2: 0.863447\n",
      "[254]\tvalid_0's l2: 0.863425\n",
      "[255]\tvalid_0's l2: 0.863419\n",
      "[256]\tvalid_0's l2: 0.863409\n",
      "[257]\tvalid_0's l2: 0.86341\n",
      "[258]\tvalid_0's l2: 0.863402\n",
      "[259]\tvalid_0's l2: 0.863394\n",
      "[260]\tvalid_0's l2: 0.863384\n",
      "[261]\tvalid_0's l2: 0.86338\n",
      "[262]\tvalid_0's l2: 0.863359\n",
      "[263]\tvalid_0's l2: 0.863351\n",
      "[264]\tvalid_0's l2: 0.863331\n",
      "[265]\tvalid_0's l2: 0.863327\n",
      "[266]\tvalid_0's l2: 0.863315\n",
      "[267]\tvalid_0's l2: 0.863307\n",
      "[268]\tvalid_0's l2: 0.863315\n",
      "[269]\tvalid_0's l2: 0.86331\n",
      "[270]\tvalid_0's l2: 0.863325\n",
      "[271]\tvalid_0's l2: 0.863317\n",
      "[272]\tvalid_0's l2: 0.863321\n",
      "[273]\tvalid_0's l2: 0.863305\n",
      "[274]\tvalid_0's l2: 0.863297\n",
      "[275]\tvalid_0's l2: 0.863294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276]\tvalid_0's l2: 0.863289\n",
      "[277]\tvalid_0's l2: 0.863299\n",
      "[278]\tvalid_0's l2: 0.863274\n",
      "[279]\tvalid_0's l2: 0.863268\n",
      "[280]\tvalid_0's l2: 0.863237\n",
      "[281]\tvalid_0's l2: 0.86322\n",
      "[282]\tvalid_0's l2: 0.863217\n",
      "[283]\tvalid_0's l2: 0.863209\n",
      "[284]\tvalid_0's l2: 0.863192\n",
      "[285]\tvalid_0's l2: 0.863189\n",
      "[286]\tvalid_0's l2: 0.863181\n",
      "[287]\tvalid_0's l2: 0.863165\n",
      "[288]\tvalid_0's l2: 0.863144\n",
      "[289]\tvalid_0's l2: 0.863118\n",
      "[290]\tvalid_0's l2: 0.863091\n",
      "[291]\tvalid_0's l2: 0.863073\n",
      "[292]\tvalid_0's l2: 0.863076\n",
      "[293]\tvalid_0's l2: 0.863061\n",
      "[294]\tvalid_0's l2: 0.863052\n",
      "[295]\tvalid_0's l2: 0.863058\n",
      "[296]\tvalid_0's l2: 0.863048\n",
      "[297]\tvalid_0's l2: 0.863037\n",
      "[298]\tvalid_0's l2: 0.863029\n",
      "[299]\tvalid_0's l2: 0.863009\n",
      "[300]\tvalid_0's l2: 0.862989\n",
      "[301]\tvalid_0's l2: 0.862973\n",
      "[302]\tvalid_0's l2: 0.86295\n",
      "[303]\tvalid_0's l2: 0.862935\n",
      "[304]\tvalid_0's l2: 0.862935\n",
      "[305]\tvalid_0's l2: 0.86292\n",
      "[306]\tvalid_0's l2: 0.862897\n",
      "[307]\tvalid_0's l2: 0.862898\n",
      "[308]\tvalid_0's l2: 0.862901\n",
      "[309]\tvalid_0's l2: 0.862903\n",
      "[310]\tvalid_0's l2: 0.862889\n",
      "[311]\tvalid_0's l2: 0.862887\n",
      "[312]\tvalid_0's l2: 0.86288\n",
      "[313]\tvalid_0's l2: 0.862861\n",
      "[314]\tvalid_0's l2: 0.862854\n",
      "[315]\tvalid_0's l2: 0.862833\n",
      "[316]\tvalid_0's l2: 0.862813\n",
      "[317]\tvalid_0's l2: 0.862793\n",
      "[318]\tvalid_0's l2: 0.862798\n",
      "[319]\tvalid_0's l2: 0.862783\n",
      "[320]\tvalid_0's l2: 0.862772\n",
      "[321]\tvalid_0's l2: 0.86278\n",
      "[322]\tvalid_0's l2: 0.86276\n",
      "[323]\tvalid_0's l2: 0.86275\n",
      "[324]\tvalid_0's l2: 0.862739\n",
      "[325]\tvalid_0's l2: 0.862725\n",
      "[326]\tvalid_0's l2: 0.862687\n",
      "[327]\tvalid_0's l2: 0.86268\n",
      "[328]\tvalid_0's l2: 0.862666\n",
      "[329]\tvalid_0's l2: 0.862678\n",
      "[330]\tvalid_0's l2: 0.862682\n",
      "[331]\tvalid_0's l2: 0.86266\n",
      "[332]\tvalid_0's l2: 0.862653\n",
      "[333]\tvalid_0's l2: 0.862653\n",
      "[334]\tvalid_0's l2: 0.862639\n",
      "[335]\tvalid_0's l2: 0.862635\n",
      "[336]\tvalid_0's l2: 0.862615\n",
      "[337]\tvalid_0's l2: 0.862584\n",
      "[338]\tvalid_0's l2: 0.862565\n",
      "[339]\tvalid_0's l2: 0.862559\n",
      "[340]\tvalid_0's l2: 0.862552\n",
      "[341]\tvalid_0's l2: 0.862555\n",
      "[342]\tvalid_0's l2: 0.862567\n",
      "[343]\tvalid_0's l2: 0.862557\n",
      "[344]\tvalid_0's l2: 0.86254\n",
      "[345]\tvalid_0's l2: 0.86249\n",
      "[346]\tvalid_0's l2: 0.862493\n",
      "[347]\tvalid_0's l2: 0.862492\n",
      "[348]\tvalid_0's l2: 0.862491\n",
      "[349]\tvalid_0's l2: 0.862476\n",
      "[350]\tvalid_0's l2: 0.862469\n",
      "[351]\tvalid_0's l2: 0.862466\n",
      "[352]\tvalid_0's l2: 0.862469\n",
      "[353]\tvalid_0's l2: 0.86246\n",
      "[354]\tvalid_0's l2: 0.862463\n",
      "[355]\tvalid_0's l2: 0.862446\n",
      "[356]\tvalid_0's l2: 0.862433\n",
      "[357]\tvalid_0's l2: 0.862427\n",
      "[358]\tvalid_0's l2: 0.862404\n",
      "[359]\tvalid_0's l2: 0.862394\n",
      "[360]\tvalid_0's l2: 0.862381\n",
      "[361]\tvalid_0's l2: 0.862372\n",
      "[362]\tvalid_0's l2: 0.862354\n",
      "[363]\tvalid_0's l2: 0.86235\n",
      "[364]\tvalid_0's l2: 0.86234\n",
      "[365]\tvalid_0's l2: 0.86233\n",
      "[366]\tvalid_0's l2: 0.862319\n",
      "[367]\tvalid_0's l2: 0.862309\n",
      "[368]\tvalid_0's l2: 0.862305\n",
      "[369]\tvalid_0's l2: 0.862289\n",
      "[370]\tvalid_0's l2: 0.862285\n",
      "[371]\tvalid_0's l2: 0.862278\n",
      "[372]\tvalid_0's l2: 0.862272\n",
      "[373]\tvalid_0's l2: 0.862258\n",
      "[374]\tvalid_0's l2: 0.862255\n",
      "[375]\tvalid_0's l2: 0.862244\n",
      "[376]\tvalid_0's l2: 0.86223\n",
      "[377]\tvalid_0's l2: 0.862215\n",
      "[378]\tvalid_0's l2: 0.862209\n",
      "[379]\tvalid_0's l2: 0.862197\n",
      "[380]\tvalid_0's l2: 0.862198\n",
      "[381]\tvalid_0's l2: 0.862203\n",
      "[382]\tvalid_0's l2: 0.862179\n",
      "[383]\tvalid_0's l2: 0.862167\n",
      "[384]\tvalid_0's l2: 0.862165\n",
      "[385]\tvalid_0's l2: 0.862153\n",
      "[386]\tvalid_0's l2: 0.862143\n",
      "[387]\tvalid_0's l2: 0.862144\n",
      "[388]\tvalid_0's l2: 0.862144\n",
      "[389]\tvalid_0's l2: 0.862135\n",
      "[390]\tvalid_0's l2: 0.862136\n",
      "[391]\tvalid_0's l2: 0.86213\n",
      "[392]\tvalid_0's l2: 0.862127\n",
      "[393]\tvalid_0's l2: 0.862118\n",
      "[394]\tvalid_0's l2: 0.862125\n",
      "[395]\tvalid_0's l2: 0.862107\n",
      "[396]\tvalid_0's l2: 0.8621\n",
      "[397]\tvalid_0's l2: 0.862079\n",
      "[398]\tvalid_0's l2: 0.862073\n",
      "[399]\tvalid_0's l2: 0.862063\n",
      "[400]\tvalid_0's l2: 0.862053\n",
      "[401]\tvalid_0's l2: 0.86205\n",
      "[402]\tvalid_0's l2: 0.862044\n",
      "[403]\tvalid_0's l2: 0.862037\n",
      "[404]\tvalid_0's l2: 0.862036\n",
      "[405]\tvalid_0's l2: 0.862005\n",
      "[406]\tvalid_0's l2: 0.862002\n",
      "[407]\tvalid_0's l2: 0.861992\n",
      "[408]\tvalid_0's l2: 0.86199\n",
      "[409]\tvalid_0's l2: 0.861978\n",
      "[410]\tvalid_0's l2: 0.861955\n",
      "[411]\tvalid_0's l2: 0.861948\n",
      "[412]\tvalid_0's l2: 0.861945\n",
      "[413]\tvalid_0's l2: 0.861933\n",
      "[414]\tvalid_0's l2: 0.861918\n",
      "[415]\tvalid_0's l2: 0.86192\n",
      "[416]\tvalid_0's l2: 0.861909\n",
      "[417]\tvalid_0's l2: 0.861886\n",
      "[418]\tvalid_0's l2: 0.861864\n",
      "[419]\tvalid_0's l2: 0.861858\n",
      "[420]\tvalid_0's l2: 0.861856\n",
      "[421]\tvalid_0's l2: 0.861868\n",
      "[422]\tvalid_0's l2: 0.861846\n",
      "[423]\tvalid_0's l2: 0.861843\n",
      "[424]\tvalid_0's l2: 0.861835\n",
      "[425]\tvalid_0's l2: 0.86183\n",
      "[426]\tvalid_0's l2: 0.861832\n",
      "[427]\tvalid_0's l2: 0.861818\n",
      "[428]\tvalid_0's l2: 0.861811\n",
      "[429]\tvalid_0's l2: 0.861794\n",
      "[430]\tvalid_0's l2: 0.861789\n",
      "[431]\tvalid_0's l2: 0.86178\n",
      "[432]\tvalid_0's l2: 0.861788\n",
      "[433]\tvalid_0's l2: 0.861779\n",
      "[434]\tvalid_0's l2: 0.86176\n",
      "[435]\tvalid_0's l2: 0.861727\n",
      "[436]\tvalid_0's l2: 0.861717\n",
      "[437]\tvalid_0's l2: 0.861699\n",
      "[438]\tvalid_0's l2: 0.861693\n",
      "[439]\tvalid_0's l2: 0.861692\n",
      "[440]\tvalid_0's l2: 0.861705\n",
      "[441]\tvalid_0's l2: 0.861692\n",
      "[442]\tvalid_0's l2: 0.861713\n",
      "[443]\tvalid_0's l2: 0.861703\n",
      "[444]\tvalid_0's l2: 0.861732\n",
      "[445]\tvalid_0's l2: 0.861703\n",
      "[446]\tvalid_0's l2: 0.861691\n",
      "[447]\tvalid_0's l2: 0.861689\n",
      "[448]\tvalid_0's l2: 0.861689\n",
      "[449]\tvalid_0's l2: 0.861691\n",
      "[450]\tvalid_0's l2: 0.861677\n",
      "[451]\tvalid_0's l2: 0.861672\n",
      "[452]\tvalid_0's l2: 0.861667\n",
      "[453]\tvalid_0's l2: 0.861648\n",
      "[454]\tvalid_0's l2: 0.861632\n",
      "[455]\tvalid_0's l2: 0.861635\n",
      "[456]\tvalid_0's l2: 0.861631\n",
      "[457]\tvalid_0's l2: 0.861624\n",
      "[458]\tvalid_0's l2: 0.861628\n",
      "[459]\tvalid_0's l2: 0.861619\n",
      "[460]\tvalid_0's l2: 0.861604\n",
      "[461]\tvalid_0's l2: 0.861599\n",
      "[462]\tvalid_0's l2: 0.861591\n",
      "[463]\tvalid_0's l2: 0.861583\n",
      "[464]\tvalid_0's l2: 0.861591\n",
      "[465]\tvalid_0's l2: 0.861609\n",
      "[466]\tvalid_0's l2: 0.861592\n",
      "[467]\tvalid_0's l2: 0.861568\n",
      "[468]\tvalid_0's l2: 0.861568\n",
      "[469]\tvalid_0's l2: 0.86157\n",
      "[470]\tvalid_0's l2: 0.861561\n",
      "[471]\tvalid_0's l2: 0.861544\n",
      "[472]\tvalid_0's l2: 0.861551\n",
      "[473]\tvalid_0's l2: 0.861544\n",
      "[474]\tvalid_0's l2: 0.861551\n",
      "[475]\tvalid_0's l2: 0.86155\n",
      "[476]\tvalid_0's l2: 0.861552\n",
      "[477]\tvalid_0's l2: 0.861541\n",
      "[478]\tvalid_0's l2: 0.861545\n",
      "[479]\tvalid_0's l2: 0.861535\n",
      "[480]\tvalid_0's l2: 0.861524\n",
      "[481]\tvalid_0's l2: 0.861536\n",
      "[482]\tvalid_0's l2: 0.861535\n",
      "[483]\tvalid_0's l2: 0.86153\n",
      "[484]\tvalid_0's l2: 0.861528\n",
      "[485]\tvalid_0's l2: 0.86152\n",
      "[486]\tvalid_0's l2: 0.861539\n",
      "[487]\tvalid_0's l2: 0.861541\n",
      "[488]\tvalid_0's l2: 0.861531\n",
      "[489]\tvalid_0's l2: 0.861534\n",
      "[490]\tvalid_0's l2: 0.861515\n",
      "[491]\tvalid_0's l2: 0.861528\n",
      "[492]\tvalid_0's l2: 0.861533\n",
      "[493]\tvalid_0's l2: 0.861518\n",
      "[494]\tvalid_0's l2: 0.861515\n",
      "[495]\tvalid_0's l2: 0.861515\n",
      "[496]\tvalid_0's l2: 0.861508\n",
      "[497]\tvalid_0's l2: 0.861501\n",
      "[498]\tvalid_0's l2: 0.861481\n",
      "[499]\tvalid_0's l2: 0.861476\n",
      "[500]\tvalid_0's l2: 0.861466\n",
      "[501]\tvalid_0's l2: 0.861484\n",
      "[502]\tvalid_0's l2: 0.861469\n",
      "[503]\tvalid_0's l2: 0.861455\n",
      "[504]\tvalid_0's l2: 0.861441\n",
      "[505]\tvalid_0's l2: 0.861422\n",
      "[506]\tvalid_0's l2: 0.861419\n",
      "[507]\tvalid_0's l2: 0.861406\n",
      "[508]\tvalid_0's l2: 0.861402\n",
      "[509]\tvalid_0's l2: 0.861393\n",
      "[510]\tvalid_0's l2: 0.861379\n",
      "[511]\tvalid_0's l2: 0.861369\n",
      "[512]\tvalid_0's l2: 0.861351\n",
      "[513]\tvalid_0's l2: 0.861351\n",
      "[514]\tvalid_0's l2: 0.861357\n",
      "[515]\tvalid_0's l2: 0.861347\n",
      "[516]\tvalid_0's l2: 0.861353\n",
      "[517]\tvalid_0's l2: 0.861339\n",
      "[518]\tvalid_0's l2: 0.861334\n",
      "[519]\tvalid_0's l2: 0.861334\n",
      "[520]\tvalid_0's l2: 0.861339\n",
      "[521]\tvalid_0's l2: 0.861331\n",
      "[522]\tvalid_0's l2: 0.861333\n",
      "[523]\tvalid_0's l2: 0.861332\n",
      "[524]\tvalid_0's l2: 0.861323\n",
      "[525]\tvalid_0's l2: 0.861327\n",
      "[526]\tvalid_0's l2: 0.861346\n",
      "[527]\tvalid_0's l2: 0.861347\n",
      "[528]\tvalid_0's l2: 0.861327\n",
      "[529]\tvalid_0's l2: 0.861307\n",
      "[530]\tvalid_0's l2: 0.861293\n",
      "[531]\tvalid_0's l2: 0.8613\n",
      "[532]\tvalid_0's l2: 0.86129\n",
      "[533]\tvalid_0's l2: 0.861287\n",
      "[534]\tvalid_0's l2: 0.861292\n",
      "[535]\tvalid_0's l2: 0.861294\n",
      "[536]\tvalid_0's l2: 0.86128\n",
      "[537]\tvalid_0's l2: 0.86128\n",
      "[538]\tvalid_0's l2: 0.861266\n",
      "[539]\tvalid_0's l2: 0.861269\n",
      "[540]\tvalid_0's l2: 0.861254\n",
      "[541]\tvalid_0's l2: 0.861249\n",
      "[542]\tvalid_0's l2: 0.861244\n",
      "[543]\tvalid_0's l2: 0.861239\n",
      "[544]\tvalid_0's l2: 0.861242\n",
      "[545]\tvalid_0's l2: 0.86125\n",
      "[546]\tvalid_0's l2: 0.861245\n",
      "[547]\tvalid_0's l2: 0.861248\n",
      "[548]\tvalid_0's l2: 0.861228\n",
      "[549]\tvalid_0's l2: 0.861231\n",
      "[550]\tvalid_0's l2: 0.861234\n",
      "[551]\tvalid_0's l2: 0.861227\n",
      "[552]\tvalid_0's l2: 0.861231\n",
      "[553]\tvalid_0's l2: 0.86123\n",
      "[554]\tvalid_0's l2: 0.861221\n",
      "[555]\tvalid_0's l2: 0.86122\n",
      "[556]\tvalid_0's l2: 0.86122\n",
      "[557]\tvalid_0's l2: 0.861237\n",
      "[558]\tvalid_0's l2: 0.86123\n",
      "[559]\tvalid_0's l2: 0.861215\n",
      "[560]\tvalid_0's l2: 0.861192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[561]\tvalid_0's l2: 0.861192\n",
      "[562]\tvalid_0's l2: 0.861175\n",
      "[563]\tvalid_0's l2: 0.861154\n",
      "[564]\tvalid_0's l2: 0.861155\n",
      "[565]\tvalid_0's l2: 0.861136\n",
      "[566]\tvalid_0's l2: 0.861134\n",
      "[567]\tvalid_0's l2: 0.861132\n",
      "[568]\tvalid_0's l2: 0.861125\n",
      "[569]\tvalid_0's l2: 0.861114\n",
      "[570]\tvalid_0's l2: 0.861098\n",
      "[571]\tvalid_0's l2: 0.861094\n",
      "[572]\tvalid_0's l2: 0.861075\n",
      "[573]\tvalid_0's l2: 0.86108\n",
      "[574]\tvalid_0's l2: 0.861082\n",
      "[575]\tvalid_0's l2: 0.861074\n",
      "[576]\tvalid_0's l2: 0.861073\n",
      "[577]\tvalid_0's l2: 0.861069\n",
      "[578]\tvalid_0's l2: 0.861076\n",
      "[579]\tvalid_0's l2: 0.861064\n",
      "[580]\tvalid_0's l2: 0.861039\n",
      "[581]\tvalid_0's l2: 0.86106\n",
      "[582]\tvalid_0's l2: 0.861061\n",
      "[583]\tvalid_0's l2: 0.861055\n",
      "[584]\tvalid_0's l2: 0.861045\n",
      "[585]\tvalid_0's l2: 0.861054\n",
      "[586]\tvalid_0's l2: 0.861036\n",
      "[587]\tvalid_0's l2: 0.861021\n",
      "[588]\tvalid_0's l2: 0.861013\n",
      "[589]\tvalid_0's l2: 0.861004\n",
      "[590]\tvalid_0's l2: 0.860991\n",
      "[591]\tvalid_0's l2: 0.860993\n",
      "[592]\tvalid_0's l2: 0.860996\n",
      "[593]\tvalid_0's l2: 0.860983\n",
      "[594]\tvalid_0's l2: 0.86098\n",
      "[595]\tvalid_0's l2: 0.860983\n",
      "[596]\tvalid_0's l2: 0.860992\n",
      "[597]\tvalid_0's l2: 0.860984\n",
      "[598]\tvalid_0's l2: 0.860973\n",
      "[599]\tvalid_0's l2: 0.860973\n",
      "[600]\tvalid_0's l2: 0.860966\n",
      "[601]\tvalid_0's l2: 0.860964\n",
      "[602]\tvalid_0's l2: 0.860961\n",
      "[603]\tvalid_0's l2: 0.860958\n",
      "[604]\tvalid_0's l2: 0.860949\n",
      "[605]\tvalid_0's l2: 0.860949\n",
      "[606]\tvalid_0's l2: 0.860944\n",
      "[607]\tvalid_0's l2: 0.860922\n",
      "[608]\tvalid_0's l2: 0.860914\n",
      "[609]\tvalid_0's l2: 0.860917\n",
      "[610]\tvalid_0's l2: 0.860913\n",
      "[611]\tvalid_0's l2: 0.860924\n",
      "[612]\tvalid_0's l2: 0.860916\n",
      "[613]\tvalid_0's l2: 0.860906\n",
      "[614]\tvalid_0's l2: 0.860909\n",
      "[615]\tvalid_0's l2: 0.860909\n",
      "[616]\tvalid_0's l2: 0.860904\n",
      "[617]\tvalid_0's l2: 0.860888\n",
      "[618]\tvalid_0's l2: 0.86089\n",
      "[619]\tvalid_0's l2: 0.860887\n",
      "[620]\tvalid_0's l2: 0.860886\n",
      "[621]\tvalid_0's l2: 0.860879\n",
      "[622]\tvalid_0's l2: 0.860882\n",
      "[623]\tvalid_0's l2: 0.860886\n",
      "[624]\tvalid_0's l2: 0.860879\n",
      "[625]\tvalid_0's l2: 0.860879\n",
      "[626]\tvalid_0's l2: 0.860884\n",
      "[627]\tvalid_0's l2: 0.860866\n",
      "[628]\tvalid_0's l2: 0.860845\n",
      "[629]\tvalid_0's l2: 0.860841\n",
      "[630]\tvalid_0's l2: 0.860841\n",
      "[631]\tvalid_0's l2: 0.860838\n",
      "[632]\tvalid_0's l2: 0.860835\n",
      "[633]\tvalid_0's l2: 0.860827\n",
      "[634]\tvalid_0's l2: 0.860826\n",
      "[635]\tvalid_0's l2: 0.860827\n",
      "[636]\tvalid_0's l2: 0.860819\n",
      "[637]\tvalid_0's l2: 0.860817\n",
      "[638]\tvalid_0's l2: 0.860823\n",
      "[639]\tvalid_0's l2: 0.86082\n",
      "[640]\tvalid_0's l2: 0.860817\n",
      "[641]\tvalid_0's l2: 0.860821\n",
      "[642]\tvalid_0's l2: 0.860819\n",
      "[643]\tvalid_0's l2: 0.860803\n",
      "[644]\tvalid_0's l2: 0.860799\n",
      "[645]\tvalid_0's l2: 0.860788\n",
      "[646]\tvalid_0's l2: 0.860783\n",
      "[647]\tvalid_0's l2: 0.860783\n",
      "[648]\tvalid_0's l2: 0.860789\n",
      "[649]\tvalid_0's l2: 0.860787\n",
      "[650]\tvalid_0's l2: 0.860776\n",
      "[651]\tvalid_0's l2: 0.860782\n",
      "[652]\tvalid_0's l2: 0.860788\n",
      "[653]\tvalid_0's l2: 0.860777\n",
      "[654]\tvalid_0's l2: 0.860775\n",
      "[655]\tvalid_0's l2: 0.860764\n",
      "[656]\tvalid_0's l2: 0.860771\n",
      "[657]\tvalid_0's l2: 0.860774\n",
      "[658]\tvalid_0's l2: 0.860751\n",
      "[659]\tvalid_0's l2: 0.860753\n",
      "[660]\tvalid_0's l2: 0.860752\n",
      "[661]\tvalid_0's l2: 0.860757\n",
      "[662]\tvalid_0's l2: 0.86076\n",
      "[663]\tvalid_0's l2: 0.860757\n",
      "[664]\tvalid_0's l2: 0.860757\n",
      "[665]\tvalid_0's l2: 0.86075\n",
      "[666]\tvalid_0's l2: 0.860733\n",
      "[667]\tvalid_0's l2: 0.860726\n",
      "[668]\tvalid_0's l2: 0.860732\n",
      "[669]\tvalid_0's l2: 0.860738\n",
      "[670]\tvalid_0's l2: 0.860732\n",
      "[671]\tvalid_0's l2: 0.86074\n",
      "[672]\tvalid_0's l2: 0.860736\n",
      "[673]\tvalid_0's l2: 0.860744\n",
      "[674]\tvalid_0's l2: 0.860728\n",
      "[675]\tvalid_0's l2: 0.86072\n",
      "[676]\tvalid_0's l2: 0.86072\n",
      "[677]\tvalid_0's l2: 0.860717\n",
      "[678]\tvalid_0's l2: 0.860701\n",
      "[679]\tvalid_0's l2: 0.860697\n",
      "[680]\tvalid_0's l2: 0.860697\n",
      "[681]\tvalid_0's l2: 0.860684\n",
      "[682]\tvalid_0's l2: 0.860688\n",
      "[683]\tvalid_0's l2: 0.860695\n",
      "[684]\tvalid_0's l2: 0.860688\n",
      "[685]\tvalid_0's l2: 0.860704\n",
      "[686]\tvalid_0's l2: 0.860698\n",
      "[687]\tvalid_0's l2: 0.860694\n",
      "[688]\tvalid_0's l2: 0.860674\n",
      "[689]\tvalid_0's l2: 0.860662\n",
      "[690]\tvalid_0's l2: 0.860655\n",
      "[691]\tvalid_0's l2: 0.860661\n",
      "[692]\tvalid_0's l2: 0.86065\n",
      "[693]\tvalid_0's l2: 0.860659\n",
      "[694]\tvalid_0's l2: 0.860665\n",
      "[695]\tvalid_0's l2: 0.860666\n",
      "[696]\tvalid_0's l2: 0.860661\n",
      "[697]\tvalid_0's l2: 0.860663\n",
      "[698]\tvalid_0's l2: 0.860658\n",
      "[699]\tvalid_0's l2: 0.86066\n",
      "[700]\tvalid_0's l2: 0.860656\n",
      "[701]\tvalid_0's l2: 0.860667\n",
      "[702]\tvalid_0's l2: 0.860661\n",
      "Early stopping, best iteration is:\n",
      "[692]\tvalid_0's l2: 0.86065\n",
      "Validation Pearsonr score : 0.1141\n"
     ]
    }
   ],
   "source": [
    "class_param = 10000\n",
    "lgb_train = lgb.Dataset(X_train, (Y_train.values*class_param).astype('int')/class_param)\n",
    "lgb_eval = lgb.Dataset(X_val, (Y_val.values*class_param).astype('int')/class_param, reference=lgb_train)\n",
    "# lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "# lgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n",
    "\n",
    "params = {'seed': 1,\n",
    "           'objective': \"regression\",\n",
    "#            'objective': \"multiclass\",\n",
    "#            'num_class':10000,\n",
    "           'alpha':0.1, #default = 0\n",
    "           'lambda':1, #default = 1\n",
    "           'learning_rate': 0.02,\n",
    "           'bagging_fraction': 0.2,\n",
    "           'bagging_freq': 1,\n",
    "#            'feature_fraction': 0.3, #\n",
    "           'max_depth': 10,\n",
    "           'min_child_samples': 20, #\n",
    "           'num_leaves': 32}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=19450815,\n",
    "                valid_sets=lgb_eval,\n",
    "                #verbose_eval=False,\n",
    "                early_stopping_rounds=10,\n",
    "                )\n",
    "\n",
    "Y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "score_tuple = pearsonr(Y_test, Y_pred)\n",
    "score = score_tuple[0]\n",
    "print(f\"Validation Pearsonr score : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 0.844044\tvalid_0's l2: 0.84484\n",
      "[2]\ttraining's l2: 0.842526\tvalid_0's l2: 0.843867\n",
      "[3]\ttraining's l2: 0.841213\tvalid_0's l2: 0.843092\n",
      "[4]\ttraining's l2: 0.840081\tvalid_0's l2: 0.842474\n",
      "[5]\ttraining's l2: 0.839035\tvalid_0's l2: 0.84201\n",
      "[6]\ttraining's l2: 0.838098\tvalid_0's l2: 0.841445\n",
      "[7]\ttraining's l2: 0.837228\tvalid_0's l2: 0.84112\n",
      "[8]\ttraining's l2: 0.836446\tvalid_0's l2: 0.84088\n",
      "[9]\ttraining's l2: 0.835744\tvalid_0's l2: 0.840537\n",
      "[10]\ttraining's l2: 0.83513\tvalid_0's l2: 0.840222\n",
      "[11]\ttraining's l2: 0.834465\tvalid_0's l2: 0.839977\n",
      "[12]\ttraining's l2: 0.833892\tvalid_0's l2: 0.839717\n",
      "[13]\ttraining's l2: 0.833363\tvalid_0's l2: 0.839578\n",
      "[14]\ttraining's l2: 0.832864\tvalid_0's l2: 0.839468\n",
      "[15]\ttraining's l2: 0.832325\tvalid_0's l2: 0.839288\n",
      "[16]\ttraining's l2: 0.831847\tvalid_0's l2: 0.839128\n",
      "[17]\ttraining's l2: 0.831379\tvalid_0's l2: 0.838956\n",
      "[18]\ttraining's l2: 0.830943\tvalid_0's l2: 0.838936\n",
      "[19]\ttraining's l2: 0.830521\tvalid_0's l2: 0.83885\n",
      "[20]\ttraining's l2: 0.830103\tvalid_0's l2: 0.838711\n",
      "[21]\ttraining's l2: 0.829726\tvalid_0's l2: 0.838558\n",
      "[22]\ttraining's l2: 0.82934\tvalid_0's l2: 0.838482\n",
      "[23]\ttraining's l2: 0.828921\tvalid_0's l2: 0.838425\n",
      "[24]\ttraining's l2: 0.828561\tvalid_0's l2: 0.83833\n",
      "[25]\ttraining's l2: 0.828179\tvalid_0's l2: 0.838284\n",
      "[26]\ttraining's l2: 0.827843\tvalid_0's l2: 0.838214\n",
      "[27]\ttraining's l2: 0.827467\tvalid_0's l2: 0.83816\n",
      "[28]\ttraining's l2: 0.827126\tvalid_0's l2: 0.838107\n",
      "[29]\ttraining's l2: 0.826711\tvalid_0's l2: 0.838111\n",
      "[30]\ttraining's l2: 0.82636\tvalid_0's l2: 0.838026\n",
      "[31]\ttraining's l2: 0.826016\tvalid_0's l2: 0.837962\n",
      "[32]\ttraining's l2: 0.825709\tvalid_0's l2: 0.837877\n",
      "[33]\ttraining's l2: 0.825389\tvalid_0's l2: 0.837818\n",
      "[34]\ttraining's l2: 0.825096\tvalid_0's l2: 0.837741\n",
      "[35]\ttraining's l2: 0.824786\tvalid_0's l2: 0.837691\n",
      "[36]\ttraining's l2: 0.82445\tvalid_0's l2: 0.837655\n",
      "[37]\ttraining's l2: 0.824111\tvalid_0's l2: 0.837644\n",
      "[38]\ttraining's l2: 0.82374\tvalid_0's l2: 0.83755\n",
      "[39]\ttraining's l2: 0.823429\tvalid_0's l2: 0.837491\n",
      "[40]\ttraining's l2: 0.823173\tvalid_0's l2: 0.837403\n",
      "[41]\ttraining's l2: 0.822783\tvalid_0's l2: 0.837385\n",
      "[42]\ttraining's l2: 0.822479\tvalid_0's l2: 0.837287\n",
      "[43]\ttraining's l2: 0.822158\tvalid_0's l2: 0.837196\n",
      "[44]\ttraining's l2: 0.821908\tvalid_0's l2: 0.837228\n",
      "[45]\ttraining's l2: 0.821636\tvalid_0's l2: 0.837169\n",
      "[46]\ttraining's l2: 0.821372\tvalid_0's l2: 0.837105\n",
      "[47]\ttraining's l2: 0.821129\tvalid_0's l2: 0.837006\n",
      "[48]\ttraining's l2: 0.820815\tvalid_0's l2: 0.836932\n",
      "[49]\ttraining's l2: 0.820596\tvalid_0's l2: 0.836852\n",
      "[50]\ttraining's l2: 0.820339\tvalid_0's l2: 0.836818\n",
      "[51]\ttraining's l2: 0.82004\tvalid_0's l2: 0.83683\n",
      "[52]\ttraining's l2: 0.819801\tvalid_0's l2: 0.836814\n",
      "[53]\ttraining's l2: 0.819465\tvalid_0's l2: 0.836729\n",
      "[54]\ttraining's l2: 0.819196\tvalid_0's l2: 0.836683\n",
      "[55]\ttraining's l2: 0.818959\tvalid_0's l2: 0.836702\n",
      "[56]\ttraining's l2: 0.818648\tvalid_0's l2: 0.836675\n",
      "[57]\ttraining's l2: 0.818371\tvalid_0's l2: 0.836607\n",
      "[58]\ttraining's l2: 0.818159\tvalid_0's l2: 0.836576\n",
      "[59]\ttraining's l2: 0.81788\tvalid_0's l2: 0.836535\n",
      "[60]\ttraining's l2: 0.817653\tvalid_0's l2: 0.836519\n",
      "[61]\ttraining's l2: 0.817433\tvalid_0's l2: 0.836514\n",
      "[62]\ttraining's l2: 0.81721\tvalid_0's l2: 0.836478\n",
      "[63]\ttraining's l2: 0.81699\tvalid_0's l2: 0.836473\n",
      "[64]\ttraining's l2: 0.816752\tvalid_0's l2: 0.836428\n",
      "[65]\ttraining's l2: 0.816519\tvalid_0's l2: 0.836387\n",
      "[66]\ttraining's l2: 0.816222\tvalid_0's l2: 0.836265\n",
      "[67]\ttraining's l2: 0.816006\tvalid_0's l2: 0.83625\n",
      "[68]\ttraining's l2: 0.815782\tvalid_0's l2: 0.836238\n",
      "[69]\ttraining's l2: 0.815536\tvalid_0's l2: 0.836167\n",
      "[70]\ttraining's l2: 0.815309\tvalid_0's l2: 0.836132\n",
      "[71]\ttraining's l2: 0.815049\tvalid_0's l2: 0.836152\n",
      "[72]\ttraining's l2: 0.814854\tvalid_0's l2: 0.836108\n",
      "[73]\ttraining's l2: 0.814632\tvalid_0's l2: 0.836082\n",
      "[74]\ttraining's l2: 0.814398\tvalid_0's l2: 0.836101\n",
      "[75]\ttraining's l2: 0.814114\tvalid_0's l2: 0.836066\n",
      "[76]\ttraining's l2: 0.813844\tvalid_0's l2: 0.836007\n",
      "[77]\ttraining's l2: 0.81362\tvalid_0's l2: 0.835963\n",
      "[78]\ttraining's l2: 0.813349\tvalid_0's l2: 0.835956\n",
      "[79]\ttraining's l2: 0.813119\tvalid_0's l2: 0.835924\n",
      "[80]\ttraining's l2: 0.812822\tvalid_0's l2: 0.835934\n",
      "[81]\ttraining's l2: 0.812621\tvalid_0's l2: 0.835974\n",
      "[82]\ttraining's l2: 0.812382\tvalid_0's l2: 0.835959\n",
      "[83]\ttraining's l2: 0.812164\tvalid_0's l2: 0.835968\n",
      "[84]\ttraining's l2: 0.81196\tvalid_0's l2: 0.835887\n",
      "[85]\ttraining's l2: 0.811774\tvalid_0's l2: 0.835877\n",
      "[86]\ttraining's l2: 0.811511\tvalid_0's l2: 0.835829\n",
      "[87]\ttraining's l2: 0.811306\tvalid_0's l2: 0.83577\n",
      "[88]\ttraining's l2: 0.811121\tvalid_0's l2: 0.835716\n",
      "[89]\ttraining's l2: 0.81093\tvalid_0's l2: 0.835688\n",
      "[90]\ttraining's l2: 0.810721\tvalid_0's l2: 0.835649\n",
      "[91]\ttraining's l2: 0.810536\tvalid_0's l2: 0.835597\n",
      "[92]\ttraining's l2: 0.810288\tvalid_0's l2: 0.83558\n",
      "[93]\ttraining's l2: 0.810058\tvalid_0's l2: 0.835545\n",
      "[94]\ttraining's l2: 0.809875\tvalid_0's l2: 0.835503\n",
      "[95]\ttraining's l2: 0.809668\tvalid_0's l2: 0.835481\n",
      "[96]\ttraining's l2: 0.809433\tvalid_0's l2: 0.835469\n",
      "[97]\ttraining's l2: 0.809269\tvalid_0's l2: 0.835456\n",
      "[98]\ttraining's l2: 0.809075\tvalid_0's l2: 0.835451\n",
      "[99]\ttraining's l2: 0.808895\tvalid_0's l2: 0.835436\n",
      "[100]\ttraining's l2: 0.808652\tvalid_0's l2: 0.835418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "              random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "model = lgb.LGBMRegressor(random_state = SEED)\n",
    "model.fit(X_train, Y_train, eval_set=[(X, Y), (X_train, Y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f942470f1c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb.plot_metric(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e1caa7e7be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     train_scores, valid_scores = validation_curve(estimator=model,\n\u001b[0m\u001b[1;32m     24\u001b[0m                                                   \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                   \u001b[0mparam_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv_params = {'reg_alpha': [0, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
    "             'reg_lambda': [0, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
    "             'num_leaves': [2, 4, 8, 16, 32, 64, 96, 128, 160, 192, 224, 256],\n",
    "             'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'subsample_freq': [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "             'min_child_samples': [0, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "             }\n",
    "param_scales = {'reg_alpha': 'log',\n",
    "                'reg_lambda': 'log',\n",
    "                'num_leaves': 'linear',\n",
    "                'colsample_bytree': 'linear',\n",
    "                'subsample': 'linear',\n",
    "                'subsample_freq': 'linear',\n",
    "                'min_child_samples': 'linear'\n",
    "                }\n",
    "\n",
    "# \n",
    "for i, (k, v) in enumerate(cv_params.items()):\n",
    "    train_scores, valid_scores = validation_curve(estimator=model,\n",
    "                                                  X=X, y=y,\n",
    "                                                  param_name=k,\n",
    "                                                  param_range=v,\n",
    "                                                  fit_params=fit_params,\n",
    "                                                  cv=cv, scoring=scoring,\n",
    "                                                  n_jobs=-1)\n",
    "    # \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std  = np.std(train_scores, axis=1)\n",
    "    train_center = train_mean\n",
    "    train_high = train_mean + train_std\n",
    "    train_low = train_mean - train_std\n",
    "    # \n",
    "    valid_mean = np.mean(valid_scores, axis=1)\n",
    "    valid_std  = np.std(valid_scores, axis=1)\n",
    "    valid_center = valid_mean\n",
    "    valid_high = valid_mean + valid_std\n",
    "    valid_low = valid_mean - valid_std\n",
    "    # training_scores\n",
    "    plt.plot(v, train_center, color='blue', marker='o', markersize=5, label='training score')\n",
    "    plt.fill_between(v, train_high, train_low, alpha=0.15, color='blue')\n",
    "    # validation_scores\n",
    "    plt.plot(v, valid_center, color='green', linestyle='--', marker='o', markersize=5, label='validation score')\n",
    "    plt.fill_between(v, valid_high, valid_low, alpha=0.15, color='green')\n",
    "    # param_scales\n",
    "    plt.xscale(param_scales[k])\n",
    "    # \n",
    "    plt.xlabel(k)  # \n",
    "    plt.ylabel(scoring)  # \n",
    "    plt.legend(loc='lower right')  # \n",
    "    # \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "def draw_validation_curve(X_train,Y_train,model,param_range,param_name):\n",
    "    train_scores, test_scores = validation_curve(\n",
    "                    estimator=model, \n",
    "                    X=X_train, \n",
    "                    y=Y_train, \n",
    "                    param_name=param_name, \n",
    "                    param_range=param_range,cv=9)\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.plot(param_range, train_mean, \n",
    "             color='blue', marker='o', \n",
    "             markersize=5, label='training accuracy')\n",
    "\n",
    "    plt.fill_between(param_range, train_mean + train_std,\n",
    "                     train_mean - train_std, alpha=0.15,\n",
    "                     color='blue')\n",
    "\n",
    "    plt.plot(param_range, test_mean, \n",
    "             color='green', linestyle='--', \n",
    "             marker='s', markersize=5, \n",
    "             label='validation accuracy')\n",
    "\n",
    "    plt.fill_between(param_range, \n",
    "                     test_mean + test_std,\n",
    "                     test_mean - test_std, \n",
    "                     alpha=0.15, color='green')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xscale('log')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Parameter C')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0.8, 1.0])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-37fb681c3475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model = LGBMRegressor(boosting_type='gbdt', objective='regression',\n\u001b[1;32m     20\u001b[0m                       random_state=1, n_estimators=10000)\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdraw_validation_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min_child_samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-733cd456c2cf>\u001b[0m in \u001b[0;36mdraw_validation_curve\u001b[0;34m(X_train, Y_train, model, param_range, param_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidation_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_validation_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     train_scores, test_scores = validation_curve(\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mvalidation_curve\u001b[0;34m(estimator, X, y, param_name, param_range, groups, cv, scoring, n_jobs, pre_dispatch, verbose, error_score)\u001b[0m\n\u001b[1;32m   1465\u001b[0m     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n\u001b[1;32m   1466\u001b[0m                         verbose=verbose)\n\u001b[0;32m-> 1467\u001b[0;31m     out = parallel(delayed(_fit_and_score)(\n\u001b[0m\u001b[1;32m   1468\u001b[0m         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    893\u001b[0m             callbacks=None, init_model=None):\n\u001b[1;32m    894\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n\u001b[0m\u001b[1;32m    896\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit_params = {'seed': 1,\n",
    "           'objective': \"regression\",\n",
    "           'alpha':0.1, #default = 0\n",
    "           'lambda':1, #default = 1\n",
    "           'learning_rate': 0.02,\n",
    "           'bagging_fraction': 0.2,\n",
    "           'bagging_freq': 1,\n",
    "           'max_depth': 10,\n",
    "           'min_child_samples': 20, #\n",
    "           'num_leaves': 32}\n",
    "cv_params = {'reg_alpha': [0, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
    "             'reg_lambda': [0, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
    "             'num_leaves': [2, 4, 8, 16, 32, 64, 96, 128, 160, 192, 224, 256],\n",
    "             'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'subsample_freq': [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "             'min_child_samples': [0, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "             }\n",
    "model = LGBMRegressor(boosting_type='gbdt', objective='regression',\n",
    "                      random_state=1, n_estimators=10000)\n",
    "draw_validation_curve(X_train,Y_train,model,param_range=[20],param_name='min_child_samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong type(LGBMRegressor) for weight.\nIt should be list, numpy 1-D array or pandas Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fabc627b9e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = LGBMRegressor(boosting_type='gbdt', objective='regression',\n\u001b[1;32m      2\u001b[0m                       random_state=1, n_estimators=10000)\n\u001b[0;32m----> 3\u001b[0;31m model.fit(X_train,\n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           eval_set = [(X_train,Y_train), (X_val,Y_val)])\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    893\u001b[0m             callbacks=None, init_model=None):\n\u001b[1;32m    894\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n\u001b[0m\u001b[1;32m    896\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2603\u001b[0m                 )\n\u001b[1;32m   2604\u001b[0m             \u001b[0;31m# construct booster object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2605\u001b[0;31m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2606\u001b[0m             \u001b[0;31m# copy the parameters from train_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m                 \u001b[0;31m# create train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m                 self._lazy_init(self.data, label=self.label,\n\u001b[0m\u001b[1;32m   1816\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label should not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mset_weight\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m   2183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2185\u001b[0;31m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_to_1d_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# original values can be modified at cpp side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mlist_to_1d_numpy\u001b[0;34m(data, dtype, name)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# SparseArray should be supported as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         raise TypeError(f\"Wrong type({type(data).__name__}) for {name}.\\n\"\n\u001b[0m\u001b[1;32m    190\u001b[0m                         \"It should be list, numpy 1-D array or pandas Series\")\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong type(LGBMRegressor) for weight.\nIt should be list, numpy 1-D array or pandas Series"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor(boosting_type='gbdt', objective='regression',\n",
    "                      random_state=1, n_estimators=10000)\n",
    "model.fit(X_train,\n",
    "          Y_train,model,\n",
    "          eval_set = [(X_train,Y_train), (X_val,Y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "KMeans(init='k-means++', n_init=10, max_iter=300,\n",
    "                   tol=0.0001,precompute_distances='auto', verbose=0,\n",
    "                   random_state=None, copy_x=True, n_jobs=12)\n",
    "# pred = KMeans(n_clusters=4).fit_predict(X_train.head(1000))\n",
    "fit = KMeans(n_clusters=4).fit(X_train.head(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ryo/Work/Python_Work/kaggle/ubiquent_martket/k_means_model5.sav']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "out_path_m = '/home/ryo/Work/Python_Work/kaggle/ubiquent_martket/k_means_model5.sav'\n",
    "joblib.dump(fit, out_path_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(out_path_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 1, 1, 2, 3, 3, 3, 0, 2, 1, 3, 2, 1, 1, 2, 3, 2, 3, 3, 3,\n",
       "       3, 2, 3, 3, 3, 3, 2, 1, 1, 3, 2, 2, 3, 3, 2, 3, 1, 1, 2, 2, 2, 3,\n",
       "       1, 3, 3, 2, 3, 0, 3, 0, 3, 3, 0, 3, 2, 3, 2, 2, 1, 3, 1, 2, 1, 3,\n",
       "       3, 3, 0, 1, 3, 3, 3, 3, 3, 3, 2, 3, 1, 3, 0, 1, 2, 3, 1, 2, 2, 3,\n",
       "       1, 3, 1, 2, 1, 2, 0, 1, 0, 3, 3, 3, 3, 2, 1, 3, 2, 3, 0, 3, 2, 3,\n",
       "       1, 2, 1, 3, 2, 2, 2, 0, 3, 3, 1, 2, 2, 1, 3, 3, 3, 2, 3, 1, 1, 1,\n",
       "       0, 1, 3, 1, 2, 3, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 3, 1, 1, 0, 3, 3,\n",
       "       1, 2, 3, 3, 3, 1, 0, 3, 2, 0, 3, 1, 1, 3, 1, 1, 3, 3, 1, 0, 2, 1,\n",
       "       1, 2, 2, 3, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 3, 1, 3, 3, 2, 3, 2, 2,\n",
       "       2, 3, 2, 1, 2, 2, 3, 1, 1, 2, 0, 1, 2, 3, 3, 3, 2, 3, 3, 3, 1, 1,\n",
       "       0, 3, 3, 1, 0, 3, 3, 2, 1, 2, 0, 0, 3, 3, 1, 3, 3, 3, 1, 3, 2, 1,\n",
       "       1, 2, 0, 1, 3, 2, 1, 1, 3, 1, 2, 3, 3, 2, 1, 1, 1, 3, 3, 3, 3, 1,\n",
       "       3, 0, 3, 3, 3, 3, 1, 3, 1, 3, 2, 1, 1, 3, 3, 1, 0, 1, 3, 2, 3, 1,\n",
       "       3, 3, 2, 1, 1, 2, 2, 3, 0, 1, 3, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3,\n",
       "       3, 3, 1, 0, 1, 1, 0, 1, 3, 1, 3, 0, 3, 3, 1, 0, 1, 2, 1, 1, 3, 3,\n",
       "       1, 1, 1, 0, 3, 1, 1, 3, 3, 3, 2, 3, 1, 1, 1, 1, 3, 2, 3, 1, 2, 3,\n",
       "       1, 3, 1, 2, 3, 1, 1, 2, 3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 1, 2, 3, 2,\n",
       "       1, 2, 2, 2, 1, 1, 1, 1, 3, 1, 1, 3, 2, 0, 2, 1, 2, 1, 3, 3, 3, 1,\n",
       "       1, 3, 0, 3, 2, 1, 2, 1, 2, 3, 0, 0, 3, 1, 3, 3, 3, 0, 1, 3, 2, 3,\n",
       "       1, 3, 3, 3, 3, 2, 1, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 0, 2, 3, 3, 3,\n",
       "       3, 1, 3, 1, 3, 3, 1, 3, 3, 1, 2, 3, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3,\n",
       "       2, 0, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 2, 1, 3, 2, 3, 2, 1, 1, 3, 3,\n",
       "       2, 2, 3, 2, 2, 3, 0, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2,\n",
       "       1, 3, 2, 0, 1, 3, 1, 1, 3, 2, 3, 2, 3, 2, 0, 0, 0, 3, 1, 1, 2, 2,\n",
       "       1, 0, 3, 2, 3, 1, 3, 3, 1, 0, 3, 2, 2, 3, 1, 3, 2, 2, 1, 3, 3, 2,\n",
       "       1, 3, 3, 3, 3, 3, 1, 3, 3, 1, 1, 2, 1, 3, 3, 2, 1, 1, 1, 2, 3, 3,\n",
       "       1, 1, 2, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 2,\n",
       "       3, 3, 0, 2, 1, 2, 2, 1, 1, 2, 2, 1, 3, 1, 0, 3, 1, 1, 3, 3, 2, 1,\n",
       "       3, 1, 1, 1, 3, 3, 3, 2, 1, 1, 3, 3, 2, 1, 3, 2, 3, 1, 0, 0, 3, 3,\n",
       "       3, 2, 2, 2, 1, 1, 0, 3, 2, 3, 1, 0, 3, 3, 2, 3, 3, 3, 2, 3, 0, 3,\n",
       "       2, 2, 1, 0, 1, 3, 2, 3, 2, 3, 3, 3, 1, 3, 3, 1, 3, 2, 1, 2, 3, 3,\n",
       "       1, 3, 2, 3, 0, 1, 3, 0, 3, 3, 1, 3, 0, 3, 0, 3, 0, 1, 2, 1, 3, 1,\n",
       "       2, 3, 3, 3, 2, 2, 3, 3, 1, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 3, 3, 3,\n",
       "       1, 2, 3, 3, 3, 3, 3, 2, 0, 1, 0, 1, 1, 3, 3, 3, 2, 1, 3, 1, 3, 1,\n",
       "       3, 2, 2, 3, 0, 3, 3, 3, 3, 1, 0, 3, 3, 1, 3, 1, 0, 2, 2, 1, 3, 2,\n",
       "       3, 3, 2, 2, 3, 2, 1, 2, 1, 1, 1, 3, 2, 3, 1, 2, 3, 3, 3, 3, 1, 0,\n",
       "       1, 1, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 1, 3, 1, 2, 3, 1,\n",
       "       0, 2, 2, 3, 2, 1, 1, 3, 1, 3, 1, 1, 3, 3, 1, 3, 2, 2, 3, 2, 1, 3,\n",
       "       2, 2, 3, 3, 3, 0, 2, 0, 2, 3, 1, 3, 3, 2, 2, 3, 2, 3, 0, 3, 1, 3,\n",
       "       1, 2, 3, 2, 2, 3, 0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 1, 3,\n",
       "       3, 2, 1, 0, 1, 1, 1, 0, 3, 3, 3, 1, 1, 2, 3, 2, 3, 3, 3, 1, 1, 1,\n",
       "       3, 1, 3, 1, 3, 1, 3, 3, 1, 1, 3, 1, 1, 1, 0, 0, 1, 3, 3, 1, 2, 3,\n",
       "       1, 3, 1, 2, 3, 0, 3, 3, 1, 2, 3, 1, 3, 3, 2, 1, 2, 3, 2, 3, 3, 2,\n",
       "       1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 1, 3, 3, 1, 1, 1, 1, 3,\n",
       "       3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 0, 0, 1, 3, 1, 3, 1, 3, 3, 1, 3,\n",
       "       0, 3, 2, 1, 2, 3, 1, 3, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X.head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'time_id', 'investment_id', 'target', 'f_0', 'f_1', 'f_2',\n",
       "       'f_3', 'f_4', 'f_5',\n",
       "       ...\n",
       "       'f_292', 'f_293', 'f_294', 'f_295', 'f_296', 'f_297', 'f_298', 'f_299',\n",
       "       'diff_average', 'k_means'],\n",
       "      dtype='object', length=306)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path2 = '/home/ryo/Work/Python_Work/kaggle/ubiquent_martket/train_low_memory_diffAve_kmeans.parquet'\n",
    "df3 = pd.read_parquet(out_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'time_id', 'investment_id', 'target', 'f_0', 'f_1', 'f_2',\n",
       "       'f_3', 'f_4', 'f_5',\n",
       "       ...\n",
       "       'f_292', 'f_293', 'f_294', 'f_295', 'f_296', 'f_297', 'f_298', 'f_299',\n",
       "       'diff_average', 'k_means'],\n",
       "      dtype='object', length=306)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features.append('time_id')\n",
    "features.append('investment_id')\n",
    "features = [f'f_{i}' for i in range(300)]\n",
    "features.append('diff_average')\n",
    "features.append('k_means')\n",
    "target = 'target'\n",
    "df_features = df3[features]\n",
    "X_train, X, Y_train, Y = train_test_split(df_features, df3[target], train_size=0.6, shuffle=False)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X, Y, train_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryo/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.135893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76760\n",
      "[LightGBM] [Info] Number of data points in the train set: 1884846, number of used features: 302\n",
      "[LightGBM] [Info] Start training from score -0.027864\n",
      "[1]\tvalid_0's l2: 0.873645\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's l2: 0.873434\n",
      "[3]\tvalid_0's l2: 0.873199\n",
      "[4]\tvalid_0's l2: 0.872981\n",
      "[5]\tvalid_0's l2: 0.872766\n",
      "[6]\tvalid_0's l2: 0.872569\n",
      "[7]\tvalid_0's l2: 0.872381\n",
      "[8]\tvalid_0's l2: 0.872209\n",
      "[9]\tvalid_0's l2: 0.872021\n",
      "[10]\tvalid_0's l2: 0.871825\n",
      "[11]\tvalid_0's l2: 0.871663\n",
      "[12]\tvalid_0's l2: 0.871506\n",
      "[13]\tvalid_0's l2: 0.871355\n",
      "[14]\tvalid_0's l2: 0.871202\n",
      "[15]\tvalid_0's l2: 0.871045\n",
      "[16]\tvalid_0's l2: 0.87089\n",
      "[17]\tvalid_0's l2: 0.870773\n",
      "[18]\tvalid_0's l2: 0.870636\n",
      "[19]\tvalid_0's l2: 0.870513\n",
      "[20]\tvalid_0's l2: 0.870395\n",
      "[21]\tvalid_0's l2: 0.87026\n",
      "[22]\tvalid_0's l2: 0.870142\n",
      "[23]\tvalid_0's l2: 0.870026\n",
      "[24]\tvalid_0's l2: 0.869915\n",
      "[25]\tvalid_0's l2: 0.869799\n",
      "[26]\tvalid_0's l2: 0.8697\n",
      "[27]\tvalid_0's l2: 0.8696\n",
      "[28]\tvalid_0's l2: 0.869484\n",
      "[29]\tvalid_0's l2: 0.869389\n",
      "[30]\tvalid_0's l2: 0.869292\n",
      "[31]\tvalid_0's l2: 0.869215\n",
      "[32]\tvalid_0's l2: 0.869116\n",
      "[33]\tvalid_0's l2: 0.869033\n",
      "[34]\tvalid_0's l2: 0.868949\n",
      "[35]\tvalid_0's l2: 0.868856\n",
      "[36]\tvalid_0's l2: 0.868769\n",
      "[37]\tvalid_0's l2: 0.8687\n",
      "[38]\tvalid_0's l2: 0.868605\n",
      "[39]\tvalid_0's l2: 0.868516\n",
      "[40]\tvalid_0's l2: 0.868442\n",
      "[41]\tvalid_0's l2: 0.868373\n",
      "[42]\tvalid_0's l2: 0.868307\n",
      "[43]\tvalid_0's l2: 0.868247\n",
      "[44]\tvalid_0's l2: 0.868186\n",
      "[45]\tvalid_0's l2: 0.868143\n",
      "[46]\tvalid_0's l2: 0.868065\n",
      "[47]\tvalid_0's l2: 0.868006\n",
      "[48]\tvalid_0's l2: 0.867943\n",
      "[49]\tvalid_0's l2: 0.867891\n",
      "[50]\tvalid_0's l2: 0.867834\n",
      "[51]\tvalid_0's l2: 0.867759\n",
      "[52]\tvalid_0's l2: 0.8677\n",
      "[53]\tvalid_0's l2: 0.867663\n",
      "[54]\tvalid_0's l2: 0.867608\n",
      "[55]\tvalid_0's l2: 0.867559\n",
      "[56]\tvalid_0's l2: 0.867505\n",
      "[57]\tvalid_0's l2: 0.867461\n",
      "[58]\tvalid_0's l2: 0.86742\n",
      "[59]\tvalid_0's l2: 0.867389\n",
      "[60]\tvalid_0's l2: 0.867329\n",
      "[61]\tvalid_0's l2: 0.867295\n",
      "[62]\tvalid_0's l2: 0.867257\n",
      "[63]\tvalid_0's l2: 0.867203\n",
      "[64]\tvalid_0's l2: 0.867176\n",
      "[65]\tvalid_0's l2: 0.867126\n",
      "[66]\tvalid_0's l2: 0.867098\n",
      "[67]\tvalid_0's l2: 0.867054\n",
      "[68]\tvalid_0's l2: 0.867013\n",
      "[69]\tvalid_0's l2: 0.866967\n",
      "[70]\tvalid_0's l2: 0.866922\n",
      "[71]\tvalid_0's l2: 0.866881\n",
      "[72]\tvalid_0's l2: 0.866828\n",
      "[73]\tvalid_0's l2: 0.8668\n",
      "[74]\tvalid_0's l2: 0.866744\n",
      "[75]\tvalid_0's l2: 0.866709\n",
      "[76]\tvalid_0's l2: 0.866665\n",
      "[77]\tvalid_0's l2: 0.866633\n",
      "[78]\tvalid_0's l2: 0.866605\n",
      "[79]\tvalid_0's l2: 0.866561\n",
      "[80]\tvalid_0's l2: 0.866532\n",
      "[81]\tvalid_0's l2: 0.866494\n",
      "[82]\tvalid_0's l2: 0.866456\n",
      "[83]\tvalid_0's l2: 0.866437\n",
      "[84]\tvalid_0's l2: 0.866398\n",
      "[85]\tvalid_0's l2: 0.866338\n",
      "[86]\tvalid_0's l2: 0.86632\n",
      "[87]\tvalid_0's l2: 0.866294\n",
      "[88]\tvalid_0's l2: 0.866266\n",
      "[89]\tvalid_0's l2: 0.866242\n",
      "[90]\tvalid_0's l2: 0.866214\n",
      "[91]\tvalid_0's l2: 0.866199\n",
      "[92]\tvalid_0's l2: 0.86618\n",
      "[93]\tvalid_0's l2: 0.866166\n",
      "[94]\tvalid_0's l2: 0.86615\n",
      "[95]\tvalid_0's l2: 0.866143\n",
      "[96]\tvalid_0's l2: 0.866116\n",
      "[97]\tvalid_0's l2: 0.866084\n",
      "[98]\tvalid_0's l2: 0.866055\n",
      "[99]\tvalid_0's l2: 0.866024\n",
      "[100]\tvalid_0's l2: 0.866006\n",
      "[101]\tvalid_0's l2: 0.865978\n",
      "[102]\tvalid_0's l2: 0.86595\n",
      "[103]\tvalid_0's l2: 0.865936\n",
      "[104]\tvalid_0's l2: 0.865901\n",
      "[105]\tvalid_0's l2: 0.86588\n",
      "[106]\tvalid_0's l2: 0.86585\n",
      "[107]\tvalid_0's l2: 0.865825\n",
      "[108]\tvalid_0's l2: 0.865792\n",
      "[109]\tvalid_0's l2: 0.865771\n",
      "[110]\tvalid_0's l2: 0.865739\n",
      "[111]\tvalid_0's l2: 0.865715\n",
      "[112]\tvalid_0's l2: 0.865679\n",
      "[113]\tvalid_0's l2: 0.865649\n",
      "[114]\tvalid_0's l2: 0.865629\n",
      "[115]\tvalid_0's l2: 0.865615\n",
      "[116]\tvalid_0's l2: 0.865584\n",
      "[117]\tvalid_0's l2: 0.865564\n",
      "[118]\tvalid_0's l2: 0.865558\n",
      "[119]\tvalid_0's l2: 0.865522\n",
      "[120]\tvalid_0's l2: 0.86549\n",
      "[121]\tvalid_0's l2: 0.865452\n",
      "[122]\tvalid_0's l2: 0.865456\n",
      "[123]\tvalid_0's l2: 0.865423\n",
      "[124]\tvalid_0's l2: 0.86539\n",
      "[125]\tvalid_0's l2: 0.865371\n",
      "[126]\tvalid_0's l2: 0.865343\n",
      "[127]\tvalid_0's l2: 0.865318\n",
      "[128]\tvalid_0's l2: 0.865294\n",
      "[129]\tvalid_0's l2: 0.865279\n",
      "[130]\tvalid_0's l2: 0.865263\n",
      "[131]\tvalid_0's l2: 0.865232\n",
      "[132]\tvalid_0's l2: 0.865212\n",
      "[133]\tvalid_0's l2: 0.8652\n",
      "[134]\tvalid_0's l2: 0.865183\n",
      "[135]\tvalid_0's l2: 0.865152\n",
      "[136]\tvalid_0's l2: 0.865144\n",
      "[137]\tvalid_0's l2: 0.865143\n",
      "[138]\tvalid_0's l2: 0.865112\n",
      "[139]\tvalid_0's l2: 0.865101\n",
      "[140]\tvalid_0's l2: 0.865082\n",
      "[141]\tvalid_0's l2: 0.86506\n",
      "[142]\tvalid_0's l2: 0.865052\n",
      "[143]\tvalid_0's l2: 0.865019\n",
      "[144]\tvalid_0's l2: 0.864991\n",
      "[145]\tvalid_0's l2: 0.86497\n",
      "[146]\tvalid_0's l2: 0.864965\n",
      "[147]\tvalid_0's l2: 0.864939\n",
      "[148]\tvalid_0's l2: 0.864915\n",
      "[149]\tvalid_0's l2: 0.86488\n",
      "[150]\tvalid_0's l2: 0.864869\n",
      "[151]\tvalid_0's l2: 0.864852\n",
      "[152]\tvalid_0's l2: 0.864839\n",
      "[153]\tvalid_0's l2: 0.864849\n",
      "[154]\tvalid_0's l2: 0.864834\n",
      "[155]\tvalid_0's l2: 0.864806\n",
      "[156]\tvalid_0's l2: 0.864794\n",
      "[157]\tvalid_0's l2: 0.864829\n",
      "[158]\tvalid_0's l2: 0.864804\n",
      "[159]\tvalid_0's l2: 0.864794\n",
      "[160]\tvalid_0's l2: 0.864763\n",
      "[161]\tvalid_0's l2: 0.864747\n",
      "[162]\tvalid_0's l2: 0.864721\n",
      "[163]\tvalid_0's l2: 0.864695\n",
      "[164]\tvalid_0's l2: 0.864681\n",
      "[165]\tvalid_0's l2: 0.864663\n",
      "[166]\tvalid_0's l2: 0.864635\n",
      "[167]\tvalid_0's l2: 0.864612\n",
      "[168]\tvalid_0's l2: 0.864585\n",
      "[169]\tvalid_0's l2: 0.864573\n",
      "[170]\tvalid_0's l2: 0.864572\n",
      "[171]\tvalid_0's l2: 0.864578\n",
      "[172]\tvalid_0's l2: 0.864588\n",
      "[173]\tvalid_0's l2: 0.864574\n",
      "[174]\tvalid_0's l2: 0.864551\n",
      "[175]\tvalid_0's l2: 0.864547\n",
      "[176]\tvalid_0's l2: 0.864524\n",
      "[177]\tvalid_0's l2: 0.864509\n",
      "[178]\tvalid_0's l2: 0.86449\n",
      "[179]\tvalid_0's l2: 0.864469\n",
      "[180]\tvalid_0's l2: 0.86446\n",
      "[181]\tvalid_0's l2: 0.864439\n",
      "[182]\tvalid_0's l2: 0.86442\n",
      "[183]\tvalid_0's l2: 0.864396\n",
      "[184]\tvalid_0's l2: 0.86437\n",
      "[185]\tvalid_0's l2: 0.864347\n",
      "[186]\tvalid_0's l2: 0.864342\n",
      "[187]\tvalid_0's l2: 0.864325\n",
      "[188]\tvalid_0's l2: 0.864333\n",
      "[189]\tvalid_0's l2: 0.864315\n",
      "[190]\tvalid_0's l2: 0.8643\n",
      "[191]\tvalid_0's l2: 0.864274\n",
      "[192]\tvalid_0's l2: 0.864262\n",
      "[193]\tvalid_0's l2: 0.864257\n",
      "[194]\tvalid_0's l2: 0.864247\n",
      "[195]\tvalid_0's l2: 0.864221\n",
      "[196]\tvalid_0's l2: 0.864205\n",
      "[197]\tvalid_0's l2: 0.864186\n",
      "[198]\tvalid_0's l2: 0.864166\n",
      "[199]\tvalid_0's l2: 0.864153\n",
      "[200]\tvalid_0's l2: 0.864163\n",
      "[201]\tvalid_0's l2: 0.864173\n",
      "[202]\tvalid_0's l2: 0.864161\n",
      "[203]\tvalid_0's l2: 0.864128\n",
      "[204]\tvalid_0's l2: 0.86411\n",
      "[205]\tvalid_0's l2: 0.864101\n",
      "[206]\tvalid_0's l2: 0.864081\n",
      "[207]\tvalid_0's l2: 0.864086\n",
      "[208]\tvalid_0's l2: 0.86406\n",
      "[209]\tvalid_0's l2: 0.864035\n",
      "[210]\tvalid_0's l2: 0.864019\n",
      "[211]\tvalid_0's l2: 0.863993\n",
      "[212]\tvalid_0's l2: 0.863966\n",
      "[213]\tvalid_0's l2: 0.863947\n",
      "[214]\tvalid_0's l2: 0.863939\n",
      "[215]\tvalid_0's l2: 0.863918\n",
      "[216]\tvalid_0's l2: 0.863937\n",
      "[217]\tvalid_0's l2: 0.863928\n",
      "[218]\tvalid_0's l2: 0.863935\n",
      "[219]\tvalid_0's l2: 0.863925\n",
      "[220]\tvalid_0's l2: 0.863925\n",
      "[221]\tvalid_0's l2: 0.863919\n",
      "[222]\tvalid_0's l2: 0.863899\n",
      "[223]\tvalid_0's l2: 0.863877\n",
      "[224]\tvalid_0's l2: 0.863864\n",
      "[225]\tvalid_0's l2: 0.863849\n",
      "[226]\tvalid_0's l2: 0.863841\n",
      "[227]\tvalid_0's l2: 0.863835\n",
      "[228]\tvalid_0's l2: 0.863815\n",
      "[229]\tvalid_0's l2: 0.863799\n",
      "[230]\tvalid_0's l2: 0.863791\n",
      "[231]\tvalid_0's l2: 0.863784\n",
      "[232]\tvalid_0's l2: 0.863757\n",
      "[233]\tvalid_0's l2: 0.863744\n",
      "[234]\tvalid_0's l2: 0.863714\n",
      "[235]\tvalid_0's l2: 0.863692\n",
      "[236]\tvalid_0's l2: 0.863697\n",
      "[237]\tvalid_0's l2: 0.863676\n",
      "[238]\tvalid_0's l2: 0.863666\n",
      "[239]\tvalid_0's l2: 0.863655\n",
      "[240]\tvalid_0's l2: 0.86365\n",
      "[241]\tvalid_0's l2: 0.863636\n",
      "[242]\tvalid_0's l2: 0.863625\n",
      "[243]\tvalid_0's l2: 0.863631\n",
      "[244]\tvalid_0's l2: 0.86361\n",
      "[245]\tvalid_0's l2: 0.86359\n",
      "[246]\tvalid_0's l2: 0.863573\n",
      "[247]\tvalid_0's l2: 0.863557\n",
      "[248]\tvalid_0's l2: 0.863532\n",
      "[249]\tvalid_0's l2: 0.863505\n",
      "[250]\tvalid_0's l2: 0.863494\n",
      "[251]\tvalid_0's l2: 0.863476\n",
      "[252]\tvalid_0's l2: 0.863475\n",
      "[253]\tvalid_0's l2: 0.863452\n",
      "[254]\tvalid_0's l2: 0.863429\n",
      "[255]\tvalid_0's l2: 0.863405\n",
      "[256]\tvalid_0's l2: 0.863412\n",
      "[257]\tvalid_0's l2: 0.863409\n",
      "[258]\tvalid_0's l2: 0.863398\n",
      "[259]\tvalid_0's l2: 0.863394\n",
      "[260]\tvalid_0's l2: 0.863384\n",
      "[261]\tvalid_0's l2: 0.863383\n",
      "[262]\tvalid_0's l2: 0.863361\n",
      "[263]\tvalid_0's l2: 0.863357\n",
      "[264]\tvalid_0's l2: 0.863339\n",
      "[265]\tvalid_0's l2: 0.863336\n",
      "[266]\tvalid_0's l2: 0.863325\n",
      "[267]\tvalid_0's l2: 0.863321\n",
      "[268]\tvalid_0's l2: 0.863329\n",
      "[269]\tvalid_0's l2: 0.863324\n",
      "[270]\tvalid_0's l2: 0.86332\n",
      "[271]\tvalid_0's l2: 0.863306\n",
      "[272]\tvalid_0's l2: 0.863314\n",
      "[273]\tvalid_0's l2: 0.863296\n",
      "[274]\tvalid_0's l2: 0.863299\n",
      "[275]\tvalid_0's l2: 0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276]\tvalid_0's l2: 0.863292\n",
      "[277]\tvalid_0's l2: 0.863304\n",
      "[278]\tvalid_0's l2: 0.863316\n",
      "[279]\tvalid_0's l2: 0.86331\n",
      "[280]\tvalid_0's l2: 0.863285\n",
      "[281]\tvalid_0's l2: 0.863267\n",
      "[282]\tvalid_0's l2: 0.863266\n",
      "[283]\tvalid_0's l2: 0.863255\n",
      "[284]\tvalid_0's l2: 0.863247\n",
      "[285]\tvalid_0's l2: 0.863234\n",
      "[286]\tvalid_0's l2: 0.863226\n",
      "[287]\tvalid_0's l2: 0.863208\n",
      "[288]\tvalid_0's l2: 0.863209\n",
      "[289]\tvalid_0's l2: 0.86318\n",
      "[290]\tvalid_0's l2: 0.863154\n",
      "[291]\tvalid_0's l2: 0.863146\n",
      "[292]\tvalid_0's l2: 0.863154\n",
      "[293]\tvalid_0's l2: 0.863141\n",
      "[294]\tvalid_0's l2: 0.863127\n",
      "[295]\tvalid_0's l2: 0.863126\n",
      "[296]\tvalid_0's l2: 0.86311\n",
      "[297]\tvalid_0's l2: 0.863094\n",
      "[298]\tvalid_0's l2: 0.863086\n",
      "[299]\tvalid_0's l2: 0.863066\n",
      "[300]\tvalid_0's l2: 0.863051\n",
      "[301]\tvalid_0's l2: 0.863036\n",
      "[302]\tvalid_0's l2: 0.863011\n",
      "[303]\tvalid_0's l2: 0.862995\n",
      "[304]\tvalid_0's l2: 0.862995\n",
      "[305]\tvalid_0's l2: 0.862975\n",
      "[306]\tvalid_0's l2: 0.862957\n",
      "[307]\tvalid_0's l2: 0.862948\n",
      "[308]\tvalid_0's l2: 0.862946\n",
      "[309]\tvalid_0's l2: 0.862932\n",
      "[310]\tvalid_0's l2: 0.86292\n",
      "[311]\tvalid_0's l2: 0.862916\n",
      "[312]\tvalid_0's l2: 0.862906\n",
      "[313]\tvalid_0's l2: 0.862887\n",
      "[314]\tvalid_0's l2: 0.862877\n",
      "[315]\tvalid_0's l2: 0.862852\n",
      "[316]\tvalid_0's l2: 0.862832\n",
      "[317]\tvalid_0's l2: 0.862807\n",
      "[318]\tvalid_0's l2: 0.862813\n",
      "[319]\tvalid_0's l2: 0.862801\n",
      "[320]\tvalid_0's l2: 0.862776\n",
      "[321]\tvalid_0's l2: 0.862778\n",
      "[322]\tvalid_0's l2: 0.862773\n",
      "[323]\tvalid_0's l2: 0.862755\n",
      "[324]\tvalid_0's l2: 0.862737\n",
      "[325]\tvalid_0's l2: 0.862713\n",
      "[326]\tvalid_0's l2: 0.862693\n",
      "[327]\tvalid_0's l2: 0.862686\n",
      "[328]\tvalid_0's l2: 0.862673\n",
      "[329]\tvalid_0's l2: 0.862681\n",
      "[330]\tvalid_0's l2: 0.862694\n",
      "[331]\tvalid_0's l2: 0.862681\n",
      "[332]\tvalid_0's l2: 0.862672\n",
      "[333]\tvalid_0's l2: 0.862669\n",
      "[334]\tvalid_0's l2: 0.862661\n",
      "[335]\tvalid_0's l2: 0.862652\n",
      "[336]\tvalid_0's l2: 0.862632\n",
      "[337]\tvalid_0's l2: 0.862628\n",
      "[338]\tvalid_0's l2: 0.862604\n",
      "[339]\tvalid_0's l2: 0.862598\n",
      "[340]\tvalid_0's l2: 0.862595\n",
      "[341]\tvalid_0's l2: 0.862602\n",
      "[342]\tvalid_0's l2: 0.862624\n",
      "[343]\tvalid_0's l2: 0.86261\n",
      "[344]\tvalid_0's l2: 0.862609\n",
      "[345]\tvalid_0's l2: 0.862559\n",
      "[346]\tvalid_0's l2: 0.862553\n",
      "[347]\tvalid_0's l2: 0.862554\n",
      "[348]\tvalid_0's l2: 0.862548\n",
      "[349]\tvalid_0's l2: 0.862532\n",
      "[350]\tvalid_0's l2: 0.862525\n",
      "[351]\tvalid_0's l2: 0.862532\n",
      "[352]\tvalid_0's l2: 0.862534\n",
      "[353]\tvalid_0's l2: 0.86252\n",
      "[354]\tvalid_0's l2: 0.862519\n",
      "[355]\tvalid_0's l2: 0.862504\n",
      "[356]\tvalid_0's l2: 0.862491\n",
      "[357]\tvalid_0's l2: 0.862486\n",
      "[358]\tvalid_0's l2: 0.862454\n",
      "[359]\tvalid_0's l2: 0.862437\n",
      "[360]\tvalid_0's l2: 0.862426\n",
      "[361]\tvalid_0's l2: 0.862445\n",
      "[362]\tvalid_0's l2: 0.862442\n",
      "[363]\tvalid_0's l2: 0.862437\n",
      "[364]\tvalid_0's l2: 0.862435\n",
      "[365]\tvalid_0's l2: 0.862421\n",
      "[366]\tvalid_0's l2: 0.862419\n",
      "[367]\tvalid_0's l2: 0.862414\n",
      "[368]\tvalid_0's l2: 0.862413\n",
      "[369]\tvalid_0's l2: 0.862407\n",
      "[370]\tvalid_0's l2: 0.862401\n",
      "[371]\tvalid_0's l2: 0.862392\n",
      "[372]\tvalid_0's l2: 0.862386\n",
      "[373]\tvalid_0's l2: 0.862375\n",
      "[374]\tvalid_0's l2: 0.862368\n",
      "[375]\tvalid_0's l2: 0.862354\n",
      "[376]\tvalid_0's l2: 0.86235\n",
      "[377]\tvalid_0's l2: 0.862371\n",
      "[378]\tvalid_0's l2: 0.862364\n",
      "[379]\tvalid_0's l2: 0.862356\n",
      "[380]\tvalid_0's l2: 0.862333\n",
      "[381]\tvalid_0's l2: 0.862334\n",
      "[382]\tvalid_0's l2: 0.862314\n",
      "[383]\tvalid_0's l2: 0.8623\n",
      "[384]\tvalid_0's l2: 0.862312\n",
      "[385]\tvalid_0's l2: 0.862303\n",
      "[386]\tvalid_0's l2: 0.86229\n",
      "[387]\tvalid_0's l2: 0.862284\n",
      "[388]\tvalid_0's l2: 0.862279\n",
      "[389]\tvalid_0's l2: 0.862267\n",
      "[390]\tvalid_0's l2: 0.862259\n",
      "[391]\tvalid_0's l2: 0.86226\n",
      "[392]\tvalid_0's l2: 0.862258\n",
      "[393]\tvalid_0's l2: 0.862241\n",
      "[394]\tvalid_0's l2: 0.862244\n",
      "[395]\tvalid_0's l2: 0.86223\n",
      "[396]\tvalid_0's l2: 0.862223\n",
      "[397]\tvalid_0's l2: 0.862206\n",
      "[398]\tvalid_0's l2: 0.862205\n",
      "[399]\tvalid_0's l2: 0.862193\n",
      "[400]\tvalid_0's l2: 0.862185\n",
      "[401]\tvalid_0's l2: 0.86217\n",
      "[402]\tvalid_0's l2: 0.862155\n",
      "[403]\tvalid_0's l2: 0.862138\n",
      "[404]\tvalid_0's l2: 0.862144\n",
      "[405]\tvalid_0's l2: 0.862118\n",
      "[406]\tvalid_0's l2: 0.862113\n",
      "[407]\tvalid_0's l2: 0.8621\n",
      "[408]\tvalid_0's l2: 0.862093\n",
      "[409]\tvalid_0's l2: 0.862078\n",
      "[410]\tvalid_0's l2: 0.862059\n",
      "[411]\tvalid_0's l2: 0.862047\n",
      "[412]\tvalid_0's l2: 0.862032\n",
      "[413]\tvalid_0's l2: 0.862022\n",
      "[414]\tvalid_0's l2: 0.862002\n",
      "[415]\tvalid_0's l2: 0.862\n",
      "[416]\tvalid_0's l2: 0.861988\n",
      "[417]\tvalid_0's l2: 0.861977\n",
      "[418]\tvalid_0's l2: 0.861967\n",
      "[419]\tvalid_0's l2: 0.861968\n",
      "[420]\tvalid_0's l2: 0.861965\n",
      "[421]\tvalid_0's l2: 0.861976\n",
      "[422]\tvalid_0's l2: 0.861953\n",
      "[423]\tvalid_0's l2: 0.861954\n",
      "[424]\tvalid_0's l2: 0.861945\n",
      "[425]\tvalid_0's l2: 0.861938\n",
      "[426]\tvalid_0's l2: 0.861934\n",
      "[427]\tvalid_0's l2: 0.861911\n",
      "[428]\tvalid_0's l2: 0.861905\n",
      "[429]\tvalid_0's l2: 0.861897\n",
      "[430]\tvalid_0's l2: 0.86189\n",
      "[431]\tvalid_0's l2: 0.861872\n",
      "[432]\tvalid_0's l2: 0.861878\n",
      "[433]\tvalid_0's l2: 0.861868\n",
      "[434]\tvalid_0's l2: 0.861859\n",
      "[435]\tvalid_0's l2: 0.861822\n",
      "[436]\tvalid_0's l2: 0.861824\n",
      "[437]\tvalid_0's l2: 0.861818\n",
      "[438]\tvalid_0's l2: 0.861825\n",
      "[439]\tvalid_0's l2: 0.861829\n",
      "[440]\tvalid_0's l2: 0.861833\n",
      "[441]\tvalid_0's l2: 0.861827\n",
      "[442]\tvalid_0's l2: 0.861836\n",
      "[443]\tvalid_0's l2: 0.861825\n",
      "[444]\tvalid_0's l2: 0.861848\n",
      "[445]\tvalid_0's l2: 0.86183\n",
      "[446]\tvalid_0's l2: 0.861824\n",
      "[447]\tvalid_0's l2: 0.861821\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's l2: 0.861818\n",
      "Validation Pearsonr score : 0.1135\n"
     ]
    }
   ],
   "source": [
    "class_param = 10000\n",
    "lgb_train = lgb.Dataset(X_train, (Y_train.values*class_param).astype('int')/class_param)\n",
    "lgb_eval = lgb.Dataset(X_val, (Y_val.values*class_param).astype('int')/class_param, reference=lgb_train)\n",
    "# lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "# lgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n",
    "\n",
    "params = {'seed': 1,\n",
    "           'objective': \"regression\",\n",
    "#            'objective': \"multiclass\",\n",
    "#            'num_class':10000,\n",
    "           'alpha':0.1, #default = 0\n",
    "           'lambda':1, #default = 1\n",
    "           'learning_rate': 0.02,\n",
    "           'bagging_fraction': 0.2,\n",
    "           'bagging_freq': 1,\n",
    "#            'feature_fraction': 0.3, #\n",
    "           'max_depth': 10,\n",
    "           'min_child_samples': 20, #\n",
    "           'num_leaves': 32}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=19450815,\n",
    "                valid_sets=lgb_eval,\n",
    "                #verbose_eval=False,\n",
    "                early_stopping_rounds=10,\n",
    "                )\n",
    "\n",
    "Y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "score_tuple = pearsonr(Y_test, Y_pred)\n",
    "score = score_tuple[0]\n",
    "print(f\"Validation Pearsonr score : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
